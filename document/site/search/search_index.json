{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"src/3DGS/3DGS-based_Lidar_SLAM/Drivinggaussian/","title":"Drivinggaussian","text":""},{"location":"src/3DGS/3DGS-based_Outdoor_SLAM/S3PO-GS-SLAM/","title":"S3PO-GS SLAM","text":"<p>\u4e4b\u524d\u7684 3DGS SLAM \u5728\u7eaf RGB \u8f93\u5165\u7684\u5ba4\u5916\u573a\u666f\u9762\u4e34\u7f3a\u4e4f\u51e0\u4f55\u5148\u9a8c\u4fe1\u606f\uff08lacks geometric priors\uff09\u548c\u5c3a\u5ea6\u6f02\u79fb\uff08scale drift\uff09\u7684\u95ee\u9898\u3002\u4e00\u65b9\u9762\uff0c\u4e4b\u524d\u7684 3DGS SLAM \u901a\u8fc7\u5149\u5ea6\u8bef\u5dee\u7684\u53cd\u5411\u4f20\u64ad\u4f18\u5316\u76f8\u673a\u4f4d\u59ff\uff0c\u4f46\u662f\u7531\u4e8e\u7f3a\u4e4f\u51e0\u4f55\u5148\u9a8c\uff0c\u76f8\u673a\u4f4d\u59ff\u7684\u4f18\u5316\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u4e3a\u4e86\u52a0\u5f3a\u51e0\u4f55\u7ea6\u675f\uff0cPhoto-SLAM \u548c MGS-SLAM \u5206\u522b\u5f15\u5165\u72ec\u7acb\u7684 ORB-SLAM3 \u8ddf\u8e2a\u6a21\u5757\u548c Multi-View Stereo (MVS) network \u9884\u8bad\u7ec3\u6a21\u578b\u6765\u8865\u5145\u51e0\u4f55\u4fe1\u606f\uff0c\u589e\u5f3a\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u662f\u8fd9\u79cd\u7b56\u7565\u9700\u8981\u4fdd\u6301\u5916\u90e8\u6a21\u5757\u548c 3DGS \u5730\u56fe\u4e4b\u95f4\u7684\u6bd4\u4f8b\u5bf9\u9f50 \u2014\u2014 \u8fd9\u5728\u5ba4\u5185\u573a\u666f\u662f\u5bb9\u6613\u505a\u5230\u7684\uff0c\u4f46\u5728\u65cb\u8f6c\u548c\u4f4d\u79fb\u8f83\u5927\u7684\u573a\u666f\u4e0b\uff0c\u7d2f\u79ef\u8bef\u5dee\u5bb9\u6613\u5bfc\u81f4 SLAM \u7cfb\u7edf\u7684\u5c3a\u5ea6\u6f02\u79fb\uff0c\u964d\u4f4e\u540e\u7eed\u7684\u59ff\u6001\u4f30\u8ba1\u548c\u5730\u56fe\u91cd\u5efa\u8d28\u91cf\u3002</p> <p></p> <p> </p> <p>[1] Cheng C, Yu S, Wang Z, et al. Outdoor monocular slam with global scale-consistent 3d gaussian pointmaps[J]. arXiv preprint arXiv:2507.03737, 2025.</p>"},{"location":"src/3DGS/3DGS-based_Semantic_SLAM/SGS-SLAM/","title":"SGS-SLAM","text":""},{"location":"src/3DGS/3DGS-based_Visual_SLAM/GS-ICP-SLAM/","title":"GS-ICP SLAM","text":"<p>\u6700\u5f00\u59cb\u7684 SplaTAM\u3001Gaussian Splatting SLAM \u548c GS-SLAM \u90fd\u91c7\u7528\u4e86\u9ad8\u65af\u692d\u7403\u53c2\u6570\u548c\u76f8\u673a\u4f4d\u59ff\u901a\u8fc7\u5149\u5ea6\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\u8054\u5408\u4f18\u5316\u7684\u7d27\u8026\u5408\u65b9\u5f0f\uff0c\u4f46\u8fd9\u663e\u7136\u9650\u5236\u4e86 SLAM \u524d\u7aef\u7684\u8ddf\u8e2a\u901f\u5ea6\uff1b\u540e\u7eed\u7684 Photo-SLAM \u5219\u91c7\u7528\u4e86 Tracking \u7ebf\u7a0b\u590d\u7528 ORB-SLAM\u3001Mapping \u7ebf\u7a0b\u5229\u7528 3DGS \u7cbe\u7ec6\u5316\u5efa\u56fe\u7684\u677e\u8026\u5408\u67b6\u6784\u3002GS-ICP SLAM<sup>[1]</sup> \u5219\u63d0\u51fa\u8bf4 G-ICP\uff08Generalized Iterative nearest Point\uff0c\u5e7f\u4e49\u8fed\u4ee3\u6700\u8fd1\u70b9\uff09\u7b97\u6cd5\u53ef\u4ee5\u76f4\u63a5\u5c06 3DGS \u7528\u4e8e Tracking \u7ebf\u7a0b\uff0c\u5982\u6b64\u65e2\u514d\u53bb\u4e86\u7d27\u8026\u5408\u4e2d\u5149\u5ea6\u8bef\u5dee\u53cd\u5411\u4f18\u5316\u7684\u8ba1\u7b97\u91cf\uff0c\u53c8\u4e0d\u5fc5\u5982\u677e\u8026\u5408\u90a3\u822c\u5b58\u50a8\u9ad8\u65af\u5730\u56fe\u4ee5\u5916\u7684\u5197\u4f59\u7279\u5f81\u3002</p> <p></p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/GS-ICP-SLAM/#g-icp-tracking","title":"G-ICP Tracking","text":""},{"location":"src/3DGS/3DGS-based_Visual_SLAM/GS-ICP-SLAM/#gs-mapping","title":"GS Mapping","text":"<p>Tracking \u7ebf\u7a0b\u53ea\u9700\u51e0\u4e2a\u6709\u4ee3\u8868\u6027\u7684\u5173\u952e\u5e27\u5373\u53ef\uff0c\u8fc7\u591a\u7684\u5173\u952e\u5e27\u53cd\u800c\u4f1a\u4ea7\u751f\u7d2f\u79ef\u8bef\u5dee\uff0c\u4f46 Mapping \u7ebf\u7a0b\u5374\u8981\u6c42\u8f83\u591a\u7684\u5173\u952e\u5e27\u6765\u652f\u6301\u7a20\u5bc6\u5efa\u56fe\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u51b2\u7a81\uff0cGS-ICP SLAM \u4e2d Tracking \u548c Mapping \u7684\u5173\u952e\u5e27\u5e76\u4e0d\u662f\u76f8\u540c\u7684\uff0c\u800c\u662f\u5f15\u5165\u4e86 Mapping-Only Keyframe\u3002</p> <p></p> <p> </p> <p>[1] Ha S, Yeon J, Yu H. Rgbd gs-icp slam[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 180-197.</p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/Gaussian-Splatting-SLAM/","title":"Gaussian Splatting SLAM","text":"<p>\u4f5c\u4e3a CVPR 2024 Highlight &amp; Best Demo Award\uff0cGaussian Splatting SLAM \u7684 Demo \u975e\u5e38\u503c\u5f97\u4e00\u770b</p> <p></p> <p></p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/Photo-SLAM/","title":"Photo-SLAM: Monocular, Stereo and RGB-D","text":"<p>Nerf \u795e\u7ecf\u8f90\u5c04\u573a\u5bf9\u573a\u666f\u7684\u9690\u5f0f\u8868\u793a\u8bad\u7ec3\u65f6\u95f4\u957f\uff0c\u8fd9\u4e0e SLAM \u7684\u5b9e\u65f6\u6027\u8981\u6c42\u76f8\u77db\u76fe\uff0c\u800c 3DGS \u7684\u663e\u5f0f\u8868\u793a\u5374\u53ef\u80fd\u7f3a\u4e4f\u9ad8\u7ea7\u7684\u7eb9\u7406\u4fe1\u606f\uff0c\u4e0d\u5229\u4e8e\u9ad8\u7cbe\u5ea6\u5730\u56fe\u7684\u6784\u5efa\u3002Photo-SLAM<sup>[1]</sup> \u5219\u662f\u7528 Coarse-to-Fine \u7684\u65b9\u5f0f\u7ed3\u5408\u4e86\u663e\u9690\u5f0f\u8868\u8fbe\u7684\u4f18\u52bf\uff1a\u4e00\u65b9\u9762\u4f7f\u7528\u8d85\u539f\u8bed\u548c 3DGS \u6280\u672f\u6765\u83b7\u5f97\u7c97\u7cd9\u7684\u5730\u56fe\u548c\u5b9a\u4f4d\uff0c\u53e6\u4e00\u65b9\u9762\u4f7f\u7528\u589e\u91cf\u5f0f\u7684\u9ad8\u65af\u91d1\u5b57\u5854\u7f51\u7edc\u6765\u4f18\u5316\u5730\u56fe\u7ec6\u8282\u5e76\u5b66\u4e60\u573a\u666f\u7684\u9ad8\u7ea7\u7279\u5f81\u3002\u800c Photo-SLAM \u7684 Method \u6b63\u662f\u56f4\u7ed5\u4e0b\u9762\u53f3\u56fe\u7684\u4e94\u90e8\u5206\u5c55\u5f00\u7684\u3002</p> <p>Photo-SLAM is a novel framework maintaining a hyper primitives map: Explicit geometric features for efficient localization and geometry mapping solved by factor gragh, and Implicit photometric features for photorealistic mapping solved by backprogating the photometric loss, thus achieving high-quality mapping without reliance on dense depth information.</p> <p></p> <p>\u603b\u4f53\u800c\u8a00\uff0cPhoto-SLAM Tracking \u7684\u5b9e\u65f6\u6027\u5b8c\u5168\u57fa\u4e8e ORB-SLAM3 \u7684\u7279\u5f81\u70b9\u6cd5\uff0c\u53ea\u662f\u5728\u8fd9\u4e2a\u57fa\u7840\u4e0a\u591a\u5b58\u50a8\u4e86\u4e00\u4e9b\u9ad8\u65af\u692d\u7403\u53c2\u6570\uff0c\u800c Mapping \u5176\u5b9e\u4e5f\u5e76\u6ca1\u6709\u771f\u6b63\u5730\u505a\u5230 Real-Time\u3002Photo-SLAM \u5c06\u5efa\u56fe\u5206\u6210\u4e24\u90e8\u5206 \u2014\u2014 \u57fa\u4e8e\u51e0\u4f55\u4fe1\u606f\u7684\u548c\u50cf\u7d20\u7ea7\u7684\uff0c\u524d\u8005\u5b8c\u5168\u57fa\u4e8e ORB-SLAM3\uff0c\u540e\u8005\u5219\u4f1a\u6709\u4e00\u4e9b\u6ede\u540e\u6027 \u2014\u2014 \u9996\u5148\u91cd\u5efa\u51fa\u4e00\u4e2a\u201c\u7c97\u5730\u56fe\u201d\uff0c\u8fd9\u90e8\u5206\u53ea\u662f\u628a 3DGS \u539f\u6765\u901a\u8fc7 SfM \u5f97\u5230\u7684\u70b9\u66ff\u6362\u6210\u8d85\u539f\u8bed\u70b9\u8f93\u5165\u5230\u539f\u6765\u7684 3DGS \u6d41\u7a0b\u4e2d\uff0c\u4ece\u800c\u5f97\u5230\u4e00\u4e2a\u4e0d\u5305\u542b\u9ad8\u7ea7\u4fe1\u606f\u548c\u7ec6\u8282\u7684\u5730\u56fe\uff1b\u7136\u540e\u901a\u8fc7\u7f51\u7edc\u5f97\u5230\u4e00\u4e2a\u201c\u7ec6\u5730\u56fe\u201d\uff0c\u540c\u65f6\u56e0\u4e3a\u7f51\u7edc\u662f\u7528\u4e09\u5c42\u9ad8\u65af\u91d1\u5b57\u5854\u7ed3\u6784\uff0c\u6240\u4ee5\u901f\u5ea6\u4e0a\u4e0d\u4f1a\u592a\u6162\u3002\u67d0\u79cd\u610f\u4e49\u4e0a\u8bf4\uff0cORB-SLAM3 + 3DGS + \u4e09\u5c42\u91d1\u5b57\u5854\u7f51\u7edc = Photo-SLAM\uff0c\u524d\u4e24\u8005\u4fdd\u8bc1\u5b9e\u65f6\u8ddf\u8e2a\u548c\u5efa\u56fe\uff0c\u540e\u8005\u5728\u4e0d\u62d6\u5ef6\u65f6\u95f4\u7684\u57fa\u7840\u4e0a\u5c3d\u53ef\u80fd\u5730\u63d0\u5347\u5efa\u56fe\u7684\u7cbe\u5ea6<sup>[2]</sup>\u3002</p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/Photo-SLAM/#localization-and-geometry-mapping","title":"Localization and Geometry Mapping","text":"\\[ \\small\\{\\mathbf{R},\\mathbf{t}\\}=\\underset{\\mathbf{R},\\mathbf{t}}{\\arg\\min}\\sum_{i\\in\\mathcal{X}}\\rho\\left(\\|\\mathbf{p}_i-\\pi(\\mathbf{R}\\mathbf{P}_i+\\mathbf{t})\\|_{\\Sigma_g}^2\\right), \\thinspace\\thinspace\\thinspace\\thinspace\\thinspace \\{\\mathbf{P}_i,\\mathbf{R}_l,\\mathbf{t}_l|i\\in\\mathcal{P}_L,l\\in\\mathcal{K}_L\\}=\\underset{\\mathbf{P}_i,\\mathbf{R}_l,\\mathbf{t}_l}{\\arg\\min}\\sum_{\\begin{array}{c}k\\in\\mathcal{K}\\end{array}}\\sum_{\\begin{array}{c}j\\in\\mathcal{X}_k\\end{array}}\\rho(E(k,j)) \\]"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/Photo-SLAM/#photorealistic-mapping","title":"Photorealistic Mapping","text":"<p>\u50cf\u7d20\u7ea7\u7684\u5efa\u56fe\u81ea\u7136\u79bb\u4e0d\u5f00 3D-2D \u629b\u96ea\u7403\u7684\u8fc7\u7a0b\uff0c\u800c Photo-SLAM \u6cbf\u5c04\u7ebf\u7684\u4f53\u6e32\u67d3\u516c\u5f0f\u4e0e 3DGS \u7c7b\u4f3c\uff1a</p> \\[ \\small C(\\mathbf{R},\\mathbf{t})=\\sum_{i\\in \\mathcal{N}}\\mathbf{c}_i\\alpha_i\\prod_{j=1}^{i-1}(1-\\alpha_i) \\] <p>\u5176\u4e2d \\(\\small N\\) \u662f\u8d85\u57fa\u5143\u7684\u6570\u91cf\uff0c\\(\\small\\mathbf{c}_i\\) \u8868\u793a\u7531\u7403\u8c10\u7cfb\u6570 \\(\\small\\mathbf{SH}\\in\\mathbb{R}^{16}\\) \u8f6c\u6362\u5f97\u5230\u7684\u989c\u8272\uff0c\\(\\small\\alpha_i\\) \u4e3a \\(\\small\\sigma_i\\cdot\\mathcal{G}(\\mathbf{R},\\mathbf{t},\\mathbf{P}_i,\\mathbf{r}_i,\\mathbf{s}_i)\uff0c\\mathcal{G}\\) \u4ee3\u8868\u9ad8\u65af\u6cfc\u6e85\u7b97\u6cd5\u3002\u800c\u9ad8\u65af\u5747\u503c \\(\\small\\mathbf{P}\\)\u3001\u65cb\u8f6c\u77e9\u9635 \\(\\small\\mathbf{r}\\)\u3001\u7f29\u653e\u77e9\u9635 \\(\\small\\mathbf{s}\\)\u3001\u692d\u7403\u5bc6\u5ea6 \\(\\small\\sigma\\) \u548c\u7403\u8c10\u7cfb\u6570 \\(\\small\\mathbf{SH}\\) \u8fd9\u4e9b\u53c2\u6570\u7684\u4f18\u5316\u5219\u662f\u901a\u8fc7\u6700\u5c0f\u5316\u6e32\u67d3\u51fa\u7684\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u95f4\u7684\u5149\u5ea6\u8bef\u5dee\u5b9e\u73b0\u7684\u3002\u516c\u5f0f\u5982\u4e0b\u6240\u793a\uff0c\u4e5f\u53ef\u4ee5\u5728 Photo-SLAM \u4e00\u8f6e\u8bad\u7ec3\u7684\u4ee3\u7801\u4e2d\u627e\u5230\u3002</p> \\[ \\small\\mathcal{L}=(1-\\lambda)\\left|I_\\mathrm{r}-I_\\mathrm{gt}\\right|_1+\\lambda(1-\\mathrm{SSIM}(I_\\mathrm{r},I_\\mathrm{gt})) \\] <pre><code>/* src/gaussian_mapper.cpp */\nvoid GaussianMapper::trainForOneIteration()\n    ...\n    // Loss\n    auto Ll1 = loss_utils::l1_loss(masked_image, gt_image);\n    float lambda_dssim = lambdaDssim();\n    auto loss = (1.0 - lambda_dssim) * Ll1\n                + lambda_dssim * (1.0 - loss_utils::ssim(masked_image, gt_image, device_type_));\n    loss.backward();\n</code></pre> <p>\u7c7b\u4f3c\u4e8e 3DGS \u7684\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\uff0cPhoto-SLAM \u5efa\u56fe\u65f6\u81ea\u7136\u8981\u5bf9\u4e00\u5f00\u59cb\u7684\u7c97\u539f\u8bed\u4f5c\u81f4\u5bc6\u5316\u64cd\u4f5c\u3002\u6ce8\u610f\u5230 3DGS \u539f\u5148 Colmap \u521d\u59cb\u5316\u7684\u70b9\u4e91\u5bf9\u9ad8\u65af\u692d\u7403\u7684\u7a7a\u95f4\u51e0\u4f55\u7ea6\u675f\u662f\u8f83\u5f31\u7684\uff0c\u9ad8\u65af\u692d\u7403\u5728\u540e\u7eed\u4f18\u5316\u4e2d\u53ef\u4ee5\u81ea\u7531\u6d6e\u52a8\uff0c\u800c ORB-SLAM \u7684\u7279\u5f81\u70b9\u5219\u8d77\u5230\u4e86\u5bf9\u573a\u666f\u7ed3\u6784\u7684\u951a\u5b9a\u4f5c\u7528\u3002Photo-SLAM \u8ba4\u4e3a\u5206\u5e03\u5728\u56fe\u50cf\u4e0a\u7684\u4e8c\u7ef4\u7279\u5f81\u70b9\u5b9e\u9645\u4e0a\u6807\u8bb0\u51fa\u4e86\u573a\u666f\u4e2d\u5177\u6709\u590d\u6742\u7eb9\u7406\u7684\u533a\u57df\uff0c\u6240\u4ee5\u9664\u4e86 3DGS \u57fa\u4e8e\u62df\u5408\u68af\u5ea6\u7684\u514b\u9686\u548c\u5206\u88c2\u5916\uff08\u5982\u4e0b\u8ff0\u4ee3\u7801\u6240\u793a\uff09\uff0cPhoto-SLAM \u8fd8\u8003\u8651\u4e86\u5728\u7279\u5f81\u70b9\u9644\u52a0\u7684\u81f4\u5bc6\u5316\u3002\u8fd9\u79cd\u7528 ORB \u7279\u5f81\u70b9\u4f5c\u951a\u70b9\u7684\u601d\u60f3\u4e5f\u662f\u540e\u7eed\u88ab SEGS-SLAM \u6240\u501f\u9274\u7684\u4e00\u70b9\u3002</p> <pre><code>/* src/gaussian_mapper.cpp */\nvoid GaussianMapper::trainForOneIteration()\n    ...\n        // Densification\n        if (getIteration() &lt; opt_params_.densify_until_iter_) {\n            // Keep track of max radii in image-space for pruning\n            gaussians_-&gt;max_radii2D_.index_put_(\n                {visibility_filter},\n                torch::max(gaussians_-&gt;max_radii2D_.index({visibility_filter}),\n                            radii.index({visibility_filter})));\n            // if (!isdoingGausPyramidTraining() || training_level &lt; num_gaus_pyramid_sub_levels_)\n                gaussians_-&gt;addDensificationStats(viewspace_point_tensor, visibility_filter);\n\n            if ((getIteration() &gt; opt_params_.densify_from_iter_) &amp;&amp;\n                (getIteration() % densifyInterval()== 0)) {\n                int size_threshold = (getIteration() &gt; prune_big_point_after_iter_) ? 20 : 0;\n                gaussians_-&gt;densifyAndPrune(\n                    densifyGradThreshold(),\n                    densify_min_opacity_,//0.005,//\n                    scene_-&gt;cameras_extent_,\n                    size_threshold\n                );\n            }\n\n            if (opacityResetInterval()\n                &amp;&amp; (getIteration() % opacityResetInterval() == 0\n                    ||(model_params_.white_background_ &amp;&amp; getIteration() == opt_params_.densify_from_iter_)))\n                gaussians_-&gt;resetOpacity();\n        }\n\n        auto iter_end_timing = std::chrono::steady_clock::now();\n        auto iter_time = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(\n                        iter_end_timing - iter_start_timing).count();\n\n/* src/gaussian_model.cpp */\nvoid GaussianModel::densifyAndPrune(\n    float max_grad,\n    float min_opacity,\n    float extent,\n    int max_screen_size)\n{\n    auto grads = this-&gt;xyz_gradient_accum_ / this-&gt;denom_;\n    grads.index_put_({grads.isnan()}, 0.0f);\n    this-&gt;densifyAndClone(grads, max_grad, extent);\n    this-&gt;densifyAndSplit(grads, max_grad, extent);\n\n    auto prune_mask = (this-&gt;getOpacityActivation() &lt; min_opacity).squeeze();\n    if (max_screen_size) {\n        auto big_points_vs = this-&gt;max_radii2D_ &gt; max_screen_size;\n        auto big_points_ws = std::get&lt;0&gt;(this-&gt;getScalingActivation().max(/*dim=*/1)) &gt; 0.1f * extent;\n        prune_mask = torch::logical_or(torch::logical_or(prune_mask, big_points_vs), big_points_ws);\n    }\n    this-&gt;prunePoints(prune_mask);\n\n    c10::cuda::CUDACachingAllocator::emptyCache(); // torch.cuda.empty_cache()\n}\n</code></pre>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/Photo-SLAM/#gaussian-pyramid-based-learning","title":"Gaussian-Pyramid-Based Learning","text":"<p>\u5229\u7528 OEB-SLAM3 \u548c 3DGS \u5f97\u5230\u7c97\u5730\u56fe\u540e\uff0cPhoto-SLAM \u901a\u8fc7\u4e09\u5c42\u9ad8\u65af\u91d1\u5b57\u5854\u7f51\u7edc\u6765\u4f18\u5316\u5730\u56fe\u7ec6\u8282\uff0c\u4e0b\u9762\u4ee3\u7801\u9ad8\u4eae\u7684\u90e8\u5206\u5c55\u793a\u4e86\u7f51\u7edc\u5c42\u6570\u3001\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u7403\u8c10\u51fd\u6570\u9636\u6570\u7684\u8bbe\u7f6e\u3002</p> <p></p> <pre><code>/* src/gaussian_mapper.cpp */\nvoid GaussianMapper::trainForOneIteration()\n{\n    increaseIteration(1);\n    auto iter_start_timing = std::chrono::steady_clock::now();\n\n    // Pick a random Camera\n    std::shared_ptr&lt;GaussianKeyframe&gt; viewpoint_cam = useOneRandomSlidingWindowKeyframe();\n    if (!viewpoint_cam) {\n        increaseIteration(-1);\n        return;\n    }\n\n    writeKeyframeUsedTimes(result_dir_ / \"used_times\");\n\n    // if (isdoingInactiveGeoDensify() &amp;&amp; !viewpoint_cam-&gt;done_inactive_geo_densify_)\n    // increasePcdByKeyframeInactiveGeoDensify(viewpoint_cam);\n\n    int training_level = num_gaus_pyramid_sub_levels_;\n    int image_height, image_width;\n    torch::Tensor gt_image, mask;\n    if (isdoingGausPyramidTraining())\n        training_level = viewpoint_cam-&gt;getCurrentGausPyramidLevel();\n    if (training_level == num_gaus_pyramid_sub_levels_) {\n        image_height = viewpoint_cam-&gt;image_height_;\n        image_width = viewpoint_cam-&gt;image_width_;\n        gt_image = viewpoint_cam-&gt;original_image_.cuda();\n        mask = undistort_mask_[viewpoint_cam-&gt;camera_id_];\n    }\n    else {\n        image_height = viewpoint_cam-&gt;gaus_pyramid_height_[training_level];\n        image_width = viewpoint_cam-&gt;gaus_pyramid_width_[training_level];\n        gt_image = viewpoint_cam-&gt;gaus_pyramid_original_image_[training_level].cuda();\n        mask = scene_-&gt;cameras_.at(viewpoint_cam-&gt;camera_id_).gaus_pyramid_undistort_mask_[training_level];\n    }\n\n    // Mutex lock for usage of the gaussian model\n    std::unique_lock&lt;std::mutex&gt; lock_render(mutex_render_);\n\n    // Every 1000 its we increase the levels of SH up to a maximum degree\n    if (getIteration() % 1000 == 0 &amp;&amp; default_sh_ &lt; model_params_.sh_degree_)\n        default_sh_ += 1;\n    // if (isdoingGausPyramidTraining())\n    //     gaussians_-&gt;setShDegree(training_level);\n    // else\n        gaussians_-&gt;setShDegree(default_sh_);\n\n    // Update learning rate\n    if (pSLAM_) {\n        int used_times = kfs_used_times_[viewpoint_cam-&gt;fid_];\n        int step = (used_times &lt;= opt_params_.position_lr_max_steps_ ? used_times : opt_params_.position_lr_max_steps_);\n        float position_lr = gaussians_-&gt;updateLearningRate(step);\n        setPositionLearningRateInit(position_lr);\n    }\n    else {\n        gaussians_-&gt;updateLearningRate(getIteration());\n    }\n\n    gaussians_-&gt;setFeatureLearningRate(featureLearningRate());\n    gaussians_-&gt;setOpacityLearningRate(opacityLearningRate());\n    gaussians_-&gt;setScalingLearningRate(scalingLearningRate());\n    gaussians_-&gt;setRotationLearningRate(rotationLearningRate());\n\n    // Render\n    auto render_pkg = GaussianRenderer::render(\n        viewpoint_cam,\n        image_height,\n        image_width,\n        gaussians_,\n        pipe_params_,\n        background_,\n        override_color_\n    );\n    auto rendered_image = std::get&lt;0&gt;(render_pkg);\n    auto viewspace_point_tensor = std::get&lt;1&gt;(render_pkg);\n    auto visibility_filter = std::get&lt;2&gt;(render_pkg);\n    auto radii = std::get&lt;3&gt;(render_pkg);\n\n    // Get rid of black edges caused by undistortion\n    torch::Tensor masked_image = rendered_image * mask;\n\n    // Loss\n    auto Ll1 = loss_utils::l1_loss(masked_image, gt_image);\n    float lambda_dssim = lambdaDssim();\n    auto loss = (1.0 - lambda_dssim) * Ll1\n                + lambda_dssim * (1.0 - loss_utils::ssim(masked_image, gt_image, device_type_));\n    loss.backward();\n</code></pre>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/Photo-SLAM/#loop-closure","title":"Loop Closure","text":"<p>Loop Closure is crucial in SLAM because it helps address the problem of accumulated errors and drift that can occur during the localization and geometry mapping process. After detecting a closing loop, we can correct local keyframes and hyper primitives by similarity transformation. With corrected camera poses, the photorealistic mapping component can further get rid of the ghosting caused by odometry drifts and improve the mapping quality.<sup>[1]</sup></p> <p> </p> <p>[1] Huang H, Li L, Cheng H, et al. Photo-slam: Real-time simultaneous localization and photorealistic mapping for monocular stereo and rgb-d cameras[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 21584-21593.</p> <p>[2] Bilibili \u4e0a\u7684\u8fd9\u4e2a\u89c6\u9891\u7ed9\u51fa\u4e86 Photo-SLAM \u4eba\u6027\u5316\u7684\u8bb2\u89e3\uff0c\u4e5f\u662f\u6211\u8fd9\u7bc7\u7b14\u8bb0\u7684\u6765\u6e90\u3002</p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SEGS-SLAM/","title":"SEGS-SLAM: SOTA","text":"<p>SEGS-SLAM<sup>[1]</sup> \u5f3a\u8c03\u5bf9\u573a\u666f\u7ed3\u6784\u7684\u5229\u7528\uff0c\u800c\u8fd9\u662f\u88ab\u4e4b\u524d\u5927\u591a\u6570 SLAM \u7b97\u6cd5\u6240\u5ffd\u7565\u7684\u3002\u5982\u56fe 1 \u7b2c\u4e8c\u884c\u6240\u793a\uff0cMonoGS \u5728\u91cd\u5efa\u68af\u5b50\u7ed3\u6784\u65f6\u5448\u73b0\u51fa\u660e\u663e\u6742\u4e71\u7684\u7ed3\u6784\uff1bPhoto-SLAM \u867d\u7136\u5229\u7528\u4e86\u573a\u666f\u7ed3\u6784\uff0c\u5728\u5730\u56fe\u7a20\u5bc6\u5316\u4e2d\u5f15\u5165\u4e86\u51e0\u4f55\u81f4\u5bc6\u5316\u6a21\u5757\uff0c\u5176\u9ad8\u65af\u53c2\u6570\u80fd\u4ee5\u8f83\u5c11\u8fed\u4ee3\u6b21\u6570\u6536\u655b\u5230\u76f8\u5bf9\u8f83\u4f18\u7684\u7ed3\u679c\uff0c\u4f46\u4f9d\u7136\u6ca1\u6709\u5145\u5206\u5229\u7528\u573a\u666f\u7ed3\u6784\uff0c\u5982\u56fe 1 \u7b2c\u4e8c\u884c\u6240\u793a\uff0cPhoto-SLAM \u5bf9\u9f20\u6807\u8fb9\u7f18\u7684\u91cd\u5efa\u4ecd\u663e\u6a21\u7cca<sup>[2]</sup>\u3002</p> <p></p> <p>SEGS-SLAM \u7684\u6846\u67b6\u4e0e Photo-SLAM \u7c7b\u4f3c\uff0c\u90fd\u662f\u5148\u7531 ORB-SLAM3 \u751f\u6210\u76f8\u673a\u8f68\u8ff9\u548c\u70b9\u4e91\u7c97\u5730\u56fe\uff0c\u518d\u901a\u8fc7\u91d1\u5b57\u5854\u7f51\u7edc\u4f18\u5316\u5730\u56fe\u7ec6\u8282\uff08\u8fd9\u5bb9\u6613\u8ba9\u4eba\u8054\u60f3\u5230 SLAM \u524d\u540e\u7aef\u7684\u601d\u60f3\uff0c\u5373\u8ddf\u8e2a\u90e8\u5206\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u56fe\u50cf\u6570\u636e\uff0c\u800c\u5bf9\u5730\u56fe\u7684\u4f18\u5316\u5219\u6ca1\u5fc5\u8981\u5b9e\u65f6\u5730\u8ba1\u7b97\uff0c\u540e\u6bb5\u4f18\u5316\u53ef\u4ee5\u5728\u540e\u53f0\u6162\u6162\u8fdb\u884c\uff09\uff0c\u53ea\u662f SEGC-SLAM \u66f4\u5145\u5206\u5730\u5229\u7528\u4e86 ORB-SLAM3 \u91cd\u5efa\u70b9\u4e91\u7684\u7ed3\u6784\u7279\u6027\uff0c\u4e14\u4f18\u5316\u7f51\u7edc\u4ece\u9ad8\u65af\u91d1\u5b57\u5854\u53d8\u6210\u9891\u8c31\u91d1\u5b57\u5854\u4e86\u800c\u5df2\u3002\u5177\u4f53\u8bf4\u6765\uff0cSEGS-SLAM \u901a\u8fc7 ORB-SLAM3 \u7684 Tracking &amp; Mapping \u5f97\u5230\u76f8\u673a\u4f4d\u59ff\u548c\u70b9\u4e91\u540e\uff0c\u4e00\u65b9\u9762 Structure-Enhanced Photorealistic Mapping \u5c06\u70b9\u4e91\u4f53\u7d20\u5316\u5f97\u5230\u951a\u70b9\u4ee5\u4fdd\u7559\u573a\u666f\u7ed3\u6784\uff0c\u53e6\u4e00\u65b9\u9762 Appearance-from-Motion Embedding\uff08AfME\uff0c\u8fd0\u52a8\u4e2d\u5916\u89c2\u5d4c\u5165\uff09\u5efa\u7acb\u76f8\u673a\u4f4d\u59ff\u5230\u573a\u666f\u5916\u89c2\u7684\u6620\u5c04\u4ee5\u66f4\u597d\u5730\u6355\u6349\u4e0d\u540c\u89c6\u89d2\u4e0b\u573a\u666f\u7684\u5149\u5f71\u53d8\u5316\u3002\u6700\u540e\u57fa\u4e8e\u9891\u7387\u91d1\u5b57\u5854 Frequency Pyramid Regularization \u5bf9\u6e32\u67d3\u7684\u56fe\u50cf\u4f5c\u76d1\u7763\u8bad\u7ec3\uff0c\u5171\u540c\u4f18\u5316\u9ad8\u65af\u692d\u7403\u7684\u53c2\u6570\u548c AfME \u7684\u6743\u503c\u3002</p> <p></p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SEGS-SLAM/#\u7ed3\u6784\u589e\u5f3aorb-slam3-\u70b9\u4e91\u4f53\u7d20\u5316","title":"\u7ed3\u6784\u589e\u5f3a\uff1aORB-SLAM3 \u70b9\u4e91\u4f53\u7d20\u5316","text":"<p>\u5728 Figure 3(a) \u4e2d\uff0c\u53ef\u4ee5\u770b\u5230 Photo-SLAM \u4f18\u5316\u540e\u7684\u9ad8\u65af\u70b9\u4e91\u5448\u6742\u4e71\u4e00\u56e2\uff0c\u8fd9\u662f\u56e0\u4e3a colmap \u6216 ORB-SLAM \u521d\u59cb\u5316\u7684\u70b9\u4e91\u867d\u7136\u63d0\u4f9b\u4e86\u521d\u59cb\u4f4d\u7f6e\uff0c\u4f46\u8fd9\u4e9b\u9ad8\u65af\u70b9\u5728\u540e\u7eed\u4f18\u5316\u4e2d\u662f\u81ea\u7531\u6d6e\u52a8\u7684\uff0c\u5176\u7ed3\u6784\u7ea6\u675f\u5f88\u5f31\uff0c\u5bb9\u6613\u4e3a\u4e86\u62df\u5408\u50cf\u7d20\u989c\u8272\u800c\u727a\u7272\u51e0\u4f55\u7684\u51c6\u786e\u6027\u3002SEGS-SLAM \u5229\u7528\u951a\u70b9\u6765\u4fdd\u7559 ORB-SLAM \u751f\u6210\u70b9\u4e91\u7684\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f \u2014\u2014 \u6bcf\u4e2a\u951a\u70b9\u8d1f\u8d23\u7ba1\u7406\u5176\u5468\u56f4\u7684 \\(k\\) \u4e2a\u9ad8\u65af\u692d\u7403\uff0c\u5373</p> \\[ \\small\\left\\{\\mu_0,\\ldots,\\mu_{k-1}\\right\\}=\\mathbf{t}_v+\\left\\{\\mathcal{O}_0,\\ldots,\\mathcal{O}_{k-1}\\right\\}\\cdot l_v \\] <p>\u5176\u4e2d \\(\\small\\mu_j\\) \u662f\u7b2c \\(\\small j\\) \u4e2a\u9ad8\u65af\u692d\u7403\u7684\u4e2d\u5fc3\u4f4d\u7f6e\uff0c\\(\\small\\mathbf{t}_v\\) \u4e3a\u951a\u70b9\u4f4d\u7f6e\uff0c\\(\\small\\mathcal{O}_{j}\\) \u548c \\(\\small l_v\\) \u5206\u522b\u662f\u53ef\u5b66\u4e60\u7684\u504f\u79fb\u5411\u91cf\u548c\u7f29\u653e\u56e0\u5b50\u3002</p> <p></p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SEGS-SLAM/#\u5916\u89c2\u7406\u89e3\u4e0d\u540c\u89c6\u89d2\u7684\u573a\u666f\u5149\u5f71","title":"\u5916\u89c2\u7406\u89e3\uff1a\u4e0d\u540c\u89c6\u89d2\u7684\u573a\u666f\u5149\u5f71","text":""},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SEGS-SLAM/#\u6e32\u67d3\u7ec6\u8282\u9891\u57df\u91d1\u5b57\u5854\u6b63\u5219\u5316","title":"\u6e32\u67d3\u7ec6\u8282\uff1a\u9891\u57df\u91d1\u5b57\u5854\u6b63\u5219\u5316","text":"<p>SEGS-SLAM \u5229\u7528\u591a\u5c3a\u5ea6\u8868\u793a\u6765\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u3002\u8bbe \\(\\small s\\in\\mathcal{S}=\\{s_0,s_1,\\cdots,s_n\\}\\) \u4ee3\u8868\u56fe\u50cf\u7684\u7f29\u653e\u7387\uff0c\u4e8c\u7ef4\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5f97\u5230\u7684\u9891\u8c31\u4e3a \\(\\small\\mathcal{F}(I_r^s)(u,v), \\mathcal{F}(I_g^s)(u,v)\\)\uff0c\u9ad8\u9891\u635f\u5931\u7684\u5b9a\u4e49\u5982\u4e0b\u3002\u5176\u4e2d \\(\\small F_{hf,r}^s(u,v), F_{hf,g}^s(u,v)\\) \u4e3a \\(\\small H_{hf}(u,v)\\) \u9ad8\u901a\u6ee4\u6ce2\u5f97\u5230\u7684\u9ad8\u9891\u5206\u91cf\uff0c\\(\\small\\mathcal{N}=HW\\) \u8868\u793a\u56fe\u50cf\u7684\u5927\u5c0f\uff0c\\(\\small\\lambda_s\\) \u4ee3\u8868\u5404\u5c42\u91d1\u5b57\u5854\u635f\u5931\u7684\u6743\u91cd\u3002</p> \\[ \\small\\mathcal{L}_{hf}=\\sum_{s\\in\\mathcal{S}}\\frac{1}{\\mathcal{N}}\\lambda_s\\sum_{u,v}\\left|F_{hf,r}^s(u,v)-F_{hf,g}^s(u,v)\\right|, \\thinspace\\thinspace\\thinspace\\thinspace\\thinspace\\thinspace F_{hf,i}^s(u,v)=H_{hf}(u,v)\\cdot\\mathcal{F}(I_i^s)(u,v), \\thinspace\\thinspace\\thinspace i\\in\\{r,g\\} \\] <p> </p> <p>[1] Wen, Tianci et al. \u201cSEGS-SLAM: Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding.\u201d (2025).</p> <p>[2] \u201c\u7ed3\u6784\u589e\u5f3a+\u5916\u89c2\u5d4c\u5165\uff1aSEGS-SLAM\u5982\u4f55\u8ba93D\u9ad8\u65af\u5efa\u56fe\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u7167\u7247\u7ea7\u771f\u5b9e\u611f\uff1f\u201d\u8fd9\u7bc7\u5fae\u4fe1\u516c\u4f17\u53f7\u63a8\u6587\u7ed9\u51fa\u4e86\u901a\u4fd7\u7684\u8bba\u6587\u6982\u8ff0\uff0c\u53ef\u4f9b\u53c2\u8003\u3002</p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SplaTAM/","title":"SplaTAM\uff1aSplat, Track &amp; Map","text":"<p>\u4e3a\u4ec0\u4e48 3DGS \u53ef\u4ee5\u5e94\u7528\u5728 SLAM \u4e2d\u5462\uff1f\u7c97\u7cd9\u5730\u770b\uff0cSLAM \u4e2d\u6709 2D-3D \u5efa\u56fe\uff0c\u4e5f\u6709 3D-2D BA \u91cd\u6295\u5f71\u6d88\u9664\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7d2f\u79ef\u8bef\u5dee\u7684\u8fc7\u7a0b\uff0c\u8fd9\u4e0e 3DGS \u4e2d 2D-3D \u7684 SfM \u548c 3D-2D \u7684\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\u7c7b\u4f3c\u3002\u6bd5\u7adf\u662f\u9996\u7bc7\u5f00\u6e90\u7684 3DGS-SLAM \u8bba\u6587\uff0cSplaTAM<sup>[1]</sup> \u4f9d\u7136\u5728 Nerf \u9690\u5f0f\u548c 3DGS \u663e\u5f0f\u5feb\u901f\u5efa\u56fe\u7684\u8bed\u5883\u4e0b\u8c08\u9ad8\u4fdd\u771f\u7684\u573a\u666f\u91cd\u5efa \u2014\u2014 \u9690\u5f0f\u795e\u7ecf\u573a\u8868\u793a\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u4e0d\u6613\u7f16\u8f91\u3001\u4e0d\u80fd\u660e\u786e\u6a21\u62df\u7a7a\u95f4\u51e0\u4f55\u7279\u5f81\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u4e0e\u4e4b\u76f8\u5bf9\u7684\uff0cSplaTAM \u81ea\u7136\u8981\u56de\u7b54\u5982\u4f55\u5728 SLAM \u4e2d\u5d4c\u5165 3DGS \u6280\u672f\uff1a</p> <p>In this context, we explore the question, \u201cHow can one use an explicit volumetric representation to design a SLAM solution?\u201d Specifically, we use a radiance field based on 3D Gaussians to Splat (Render), Track, and Map for SLAM. We believe that this representation has the following benefits over existing map representations:</p> <ol> <li> <p>Fast rendering and rich optimization\uff1a\u53ef\u60f3\u800c\u77e5\uff0c\u9ad8\u65af\u6cfc\u6e85\u7684\u4e00\u5927\u4f18\u52bf\u4fbf\u662f\u53ef\u4ee5\u5feb\u901f\u6e32\u67d3\u573a\u666f\uff0c\u800c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8 3DGS \u7684\u6e32\u67d3\u901f\u5ea6\uff0cSplaTAM \u53bb\u9664\u4e86\u692d\u7403\u5916\u89c2\u5bf9\u89c2\u6d4b\u89c6\u89d2\u7684\u4f9d\u8d56\u53ca\u5176\u5404\u5411\u5f02\u6027\uff0c\u5373\u5c06\u7403\u8c10\u51fd\u6570\u7cfb\u6570\u77e9\u9635\u7b80\u5316\u4e3a RGB \u503c \\(\\small c\\in\\mathbb{R}^3\\)\uff0c\u5c06\u534f\u65b9\u5dee\u7b80\u5316\u4e3a\u7403\u534a\u5f84 \\(\\small r\\)\uff0c\u4ece\u800c\u5f97\u5230\u9ad8\u65af\u692d\u7403\u7684\u7b80\u5316\u8868\u793a \\(\\small f(x)=o\\exp(-||x-\\mu||^2/2r^2)\\)\uff0c\u5176\u4e2d \\(\\small\\mu\\in\\mathbb{R}^3\\) \u4e3a\u9ad8\u65af\u7403\u5747\u503c\uff0c\\(\\small o\\in[0,1]\\) \u8868\u793a\u4e0d\u900f\u660e\u5ea6\uff1b\u540c\u65f6\uff0cSplaTAM \u4ec5\u5728\u5f53\u524d\u56fe\u50cf\u7684\u53ef\u89c1\u8303\u56f4\u5185\u4f18\u5316\u9ad8\u65af\u692d\u7403\uff0c\u7c7b\u6bd4\u50cf\u7d20 \\(\\small p=(u,v)\\) \u7684\u989c\u8272\u6e32\u67d3\u516c\u5f0f \\(\\small C(p)=\\sum_{i=1}^nc_if_i(p)\\prod_{j=1}^{i-1}( 1-f_j(p) )\\)\uff0cSplaTAM \u5f15\u5165\u6df1\u5ea6\u635f\u5931 \\(\\small D(p)=\\sum_{i=1}^nd_if_i(p)\\prod_{j=1}^{i-1}( 1-f_j(p) )\\) \u548c\u50cf\u7d20\u53ef\u89c1\u6027 \\(\\small S(p)=\\sum_{i=1}^nf_i(p)\\prod_{j=1}^{i-1}( 1-f_j(p) )\\)\uff0c\u5373\u8be5\u50cf\u7d20\u662f\u5426\u5305\u542b\u5f53\u524d\u89c6\u56fe\u7684\u4fe1\u606f\u4ee5\u83b7\u5f97\u56fe\u50cf\u4e2d\u5f85\u4f18\u5316\u90e8\u5206\u7684\u8f6e\u5ed3\u3002</p> </li> <li> <p>Maps with explicit spatial extent\uff1aSplaTAM \u8ba4\u4e3a\u663e\u5f0f\u5730\u56fe\u5177\u6709\u660e\u786e\u7684\u7a7a\u95f4\u8303\u56f4\uff0c\u5e76\u53ef\u901a\u8fc7\u7b80\u5355\u5730\u6dfb\u52a0\u9ad8\u65af\u692d\u7403\u6765\u589e\u52a0\u5730\u56fe\u5bb9\u91cf\uff0c\u5141\u8bb8\u5728\u7f16\u8f91\u90e8\u5206\u573a\u666f\u65f6\u4fdd\u6301\u7167\u7247\u7ea7\u7684\u771f\u5b9e\u611f\u6e32\u67d3\u3002\u6b63\u5982\u7531\u4e0a\u9762\u7684\u50cf\u7d20\u53ef\u89c1\u6027 \\(\\small S(p)\\) \u5b9a\u4e49\u7684\u8f6e\u5ed3\u56fe\uff0c\u89c6\u56fe\u8fb9\u754c\u5bf9\u4e8e SLAM \u7684\u76f8\u673a\u8ddf\u8e2a\u663e\u7136\u662f\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u53ea\u5e0c\u671b\u5c06\u89c6\u56fe\u8303\u56f4\u5185\u7684\u9ad8\u65af\u692d\u7403\u4e0e\u65b0\u8f93\u5165\u7684 Ground Truth \u56fe\u50cf\u4f5c\u6bd4\u8f83\uff0c\u800c\u975e\u50cf\u9690\u5f0f\u5efa\u56fe\u90a3\u6837\u6bcf\u6b21\u90fd\u8981\u4f18\u5316\u5168\u5c40\u7684\u7f51\u7edc\u3002</p> </li> </ol> <p></p> <p>SplaTAM \u4e2d\u5bf9\u7403\u8c10\u51fd\u6570\u7cfb\u6570\u77e9\u9635\u7684\u7b80\u5316\u53ef\u80fd\u76f4\u63a5\u501f\u9274\u4e86 Gaussian Splatting SLAM 9.3. Effect of Spherical Harmonics (SH) \u6d88\u878d\u5b9e\u9a8c\u7684\u7ed3\u8bba\uff0c\u800c\u6df1\u5ea6\u635f\u5931\u7684\u5f15\u5165\u5219\u4e0e Gaussian Splatting SLAM 3.3.1. Tracking \u7c7b\u4f3c\u3002</p>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SplaTAM/#tracking\u53ef\u89c1\u8f6e\u5ed3\u5185\u7684\u989c\u8272\u6df1\u5ea6\u8bef\u5dee","title":"Tracking\uff1a\u53ef\u89c1\u8f6e\u5ed3\u5185\u7684\u989c\u8272\u6df1\u5ea6\u8bef\u5dee","text":"<p>SplaTAM \u4f7f\u7528\u5300\u901f\u6a21\u578b \\(\\small E_{t+1} = E_t + (E_t-E_{t-1})\\) \u521d\u59cb\u5316\u76f8\u673a\u4f4d\u59ff\uff0c\u4e5f\u5c31\u662f\u8bf4\u524d\u4e24\u5e27\u548c\u5f53\u524d\u5e27\u7684\u76f8\u673a\u4f4d\u59ff\u6784\u6210\u4e86\u7b49\u5dee\u6570\u5217\uff0c\u8fd9\u4e00\u70b9\u5728\u4e0b\u9762\u7684 <code>initialize_camera_pose</code> \u51fd\u6570\u4e2d\u662f\u663e\u800c\u6613\u89c1\u7684\u3002</p> <pre><code>''' scripts/spatam.py '''\ndef rgbd_slam(config: dict):\n    ...\n        # Initialize the camera pose for the current frame\n        if time_idx &gt; 0:\n            params = initialize_camera_pose(params, time_idx, forward_prop=config['tracking']['forward_prop'])\n\ndef initialize_camera_pose(params, curr_time_idx, forward_prop):\n    with torch.no_grad():\n        if curr_time_idx &gt; 1 and forward_prop:\n            # Initialize the camera pose for the current frame based on a constant velocity model\n            # Rotation\n            prev_rot1 = F.normalize(params['cam_unnorm_rots'][..., curr_time_idx-1].detach())\n            prev_rot2 = F.normalize(params['cam_unnorm_rots'][..., curr_time_idx-2].detach())\n            new_rot = F.normalize(prev_rot1 + (prev_rot1 - prev_rot2))\n            params['cam_unnorm_rots'][..., curr_time_idx] = new_rot.detach()\n\n            # Translation\n            prev_tran1 = params['cam_trans'][..., curr_time_idx-1].detach()\n            prev_tran2 = params['cam_trans'][..., curr_time_idx-2].detach()\n            new_tran = prev_tran1 + (prev_tran1 - prev_tran2)\n            params['cam_trans'][..., curr_time_idx] = new_tran.detach()\n\n        else:\n            # Initialize the camera pose for the current frame\n            params['cam_unnorm_rots'][..., curr_time_idx] = params['cam_unnorm_rots'][..., curr_time_idx-1].detach()\n            params['cam_trans'][..., curr_time_idx] = params['cam_trans'][..., curr_time_idx-1].detach()\n    \n    return params\n</code></pre> <p>\u81f3\u4e8e Traking \u4f18\u5316\u4f4d\u59ff\u7684\u90e8\u5206\u5219\u662f\u5728\u4fdd\u6301\u9ad8\u65af\u692d\u7403\u53c2\u6570\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\u5728\u53ef\u89c1\u8f6e\u5ed3\u56fe\u4e2d\u6700\u5c0f\u5316\u6df1\u5ea6\u548c\u989c\u8272\u635f\u5931\u51fd\u6570\uff0c\u516c\u5f0f\u5982\u4e0b\u6240\u793a\u3002\u7c7b\u6bd4 ORB-SLAM \u4e2d\u7684 Tracking \u7ebf\u7a0b\u8d1f\u8d23\u5bf9\u6bcf\u5e45\u65b0\u6765\u7684\u56fe\u50cf\u63d0\u53d6 ORB \u7279\u5f81\u70b9\uff0c\u5e76\u4e0e\u6700\u8fd1\u7684\u5173\u952e\u5e27\u8fdb\u884c\u6bd4\u8f83\uff0c\u8ba1\u7b97\u7279\u5f81\u70b9\u7684\u4f4d\u7f6e\u5e76\u7c97\u7565\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\uff0c\u67d0\u79cd\u610f\u4e49\u4e0a Tracking \u90e8\u5206\u7684\u9ad8\u65af\u692d\u7403\u5c31\u76f8\u5f53\u4e8e\u7279\u5f81\u70b9\uff0c\u4f46\u8fd9\u662f\u9ad8\u7cbe\u5ea6\u5730\u56fe\u53ca\u5176\u989c\u8272\u6df1\u5ea6\u4fe1\u606f\u6240\u5e26\u6765\u7684\u5bc6\u96c6\u578b\u4f18\u5316\u3002</p> \\[ \\small\\mathcal{L}_t = \\sum\\limits_p\\bigg( S(p)&gt;0.99 \\bigg)\\bigg( \\mathcal{L}_1\\big(D(p)\\big) + 0.5\\mathcal{L}_1\\big(C(p)\\big) \\bigg) \\] <p>\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u7684\u4ee3\u7801\u5982\u4e0b\u3002\u53e6\u5916\uff0c\u76f8\u6bd4 Gaussian Splatting SLAM \u4e2d\u5bf9\u76f8\u673a\u4f4d\u59ff\u4f18\u5316\u6700\u5c0f\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u63a8\u5bfc\uff08\u7c7b\u4f3c\u4e8e BA \u4e2d\u89c2\u6d4b\u76f8\u673a\u65b9\u7a0b\u5173\u4e8e\u76f8\u673a\u4f4d\u59ff\u4e0e\u7279\u5f81\u70b9\u4e24\u4e2a\u5bfc\u6570\u77e9\u9635\u7684\u63a8\u5bfc\uff09\uff0cSplaTAM \u5e76\u6ca1\u6709\u663e\u5f0f\u8ba1\u7b97\u76f8\u673a\u4f4d\u59ff\u7684\u68af\u5ea6\uff0c\u800c\u662f\u76f4\u63a5\u501f\u7528\u4e86 Pytorch \u7684\u81ea\u52a8\u5fae\u5206\u64cd\u4f5c \u2014\u2014 \u7ed3\u5408\u4e0b\u6587\u53ef\u77e5\uff0cSplaTAM \u7684\u7406\u8bba\u6027\u5f88\u5f31\u800c\u624b\u52a8\u8bbe\u8ba1\u7684\u89c4\u5219\u8f83\u591a\u3002</p> <pre><code>''' scripts/spatam.py '''\ndef get_loss(params, curr_data, variables, iter_time_idx, loss_weights, use_sil_for_loss,\n             sil_thres, use_l1, ignore_outlier_depth_loss, tracking=False, \n             mapping=False, do_ba=False, plot_dir=None, visualize_tracking_loss=False, tracking_iteration=None):\n    # Initialize Loss Dictionary\n    losses = {}\n\n    if tracking:\n        # Get current frame Gaussians, where only the camera pose gets gradient\n        transformed_gaussians = transform_to_frame(params, iter_time_idx, \n                                             gaussians_grad=False,\n                                             camera_grad=True)\n    elif mapping:\n        if do_ba:\n            # Get current frame Gaussians, where both camera pose and Gaussians get gradient\n            transformed_gaussians = transform_to_frame(params, iter_time_idx,\n                                                 gaussians_grad=True,\n                                                 camera_grad=True)\n        else:\n            # Get current frame Gaussians, where only the Gaussians get gradient\n            transformed_gaussians = transform_to_frame(params, iter_time_idx,\n                                                 gaussians_grad=True,\n                                                 camera_grad=False)\n    else:\n        # Get current frame Gaussians, where only the Gaussians get gradient\n        transformed_gaussians = transform_to_frame(params, iter_time_idx,\n                                             gaussians_grad=True,\n                                             camera_grad=False)\n\n    # Initialize Render Variables\n    rendervar = transformed_params2rendervar(params, transformed_gaussians)\n    depth_sil_rendervar = transformed_params2depthplussilhouette(params, curr_data['w2c'],\n                                                                 transformed_gaussians)\n\n    # RGB Rendering\n    rendervar['means2D'].retain_grad()\n    im, radius, _, = Renderer(raster_settings=curr_data['cam'])(**rendervar)\n    variables['means2D'] = rendervar['means2D']  # Gradient only accum from colour render for densification\n\n    # Depth &amp; Silhouette Rendering\n    depth_sil, _, _, = Renderer(raster_settings=curr_data['cam'])(**depth_sil_rendervar)\n    depth = depth_sil[0, :, :].unsqueeze(0)\n    silhouette = depth_sil[1, :, :]\n    presence_sil_mask = (silhouette &gt; sil_thres)\n    depth_sq = depth_sil[2, :, :].unsqueeze(0)\n    uncertainty = depth_sq - depth**2\n    uncertainty = uncertainty.detach()\n\n    # Mask with valid depth values (accounts for outlier depth values)\n    nan_mask = (~torch.isnan(depth)) &amp; (~torch.isnan(uncertainty))\n    if ignore_outlier_depth_loss:\n        depth_error = torch.abs(curr_data['depth'] - depth) * (curr_data['depth'] &gt; 0)\n        mask = (depth_error &lt; 10*depth_error.median())\n        mask = mask &amp; (curr_data['depth'] &gt; 0)\n    else:\n        mask = (curr_data['depth'] &gt; 0)\n    mask = mask &amp; nan_mask\n    # Mask with presence silhouette mask (accounts for empty space)\n    if tracking and use_sil_for_loss:\n        mask = mask &amp; presence_sil_mask\n\n    # Depth loss\n    if use_l1:\n        mask = mask.detach()\n        if tracking:\n            losses['depth'] = torch.abs(curr_data['depth'] - depth)[mask].sum()\n        else:\n            losses['depth'] = torch.abs(curr_data['depth'] - depth)[mask].mean()\n    \n    # RGB Loss\n    if tracking and (use_sil_for_loss or ignore_outlier_depth_loss):\n        color_mask = torch.tile(mask, (3, 1, 1))\n        color_mask = color_mask.detach()\n        losses['im'] = torch.abs(curr_data['im'] - im)[color_mask].sum()\n    elif tracking:\n        losses['im'] = torch.abs(curr_data['im'] - im).sum()\n    else:\n        losses['im'] = 0.8 * l1_loss_v1(im, curr_data['im']) + 0.2 * (1.0 - calc_ssim(im, curr_data['im']))\n\n    # Visualize the Diff Images\n    if tracking and visualize_tracking_loss:\n        fig, ax = plt.subplots(2, 4, figsize=(12, 6))\n        weighted_render_im = im * color_mask\n        weighted_im = curr_data['im'] * color_mask\n        weighted_render_depth = depth * mask\n        weighted_depth = curr_data['depth'] * mask\n        diff_rgb = torch.abs(weighted_render_im - weighted_im).mean(dim=0).detach().cpu()\n        diff_depth = torch.abs(weighted_render_depth - weighted_depth).mean(dim=0).detach().cpu()\n        viz_img = torch.clip(weighted_im.permute(1, 2, 0).detach().cpu(), 0, 1)\n        ax[0, 0].imshow(viz_img)\n        ax[0, 0].set_title(\"Weighted GT RGB\")\n        viz_render_img = torch.clip(weighted_render_im.permute(1, 2, 0).detach().cpu(), 0, 1)\n        ax[1, 0].imshow(viz_render_img)\n        ax[1, 0].set_title(\"Weighted Rendered RGB\")\n        ax[0, 1].imshow(weighted_depth[0].detach().cpu(), cmap=\"jet\", vmin=0, vmax=6)\n        ax[0, 1].set_title(\"Weighted GT Depth\")\n        ax[1, 1].imshow(weighted_render_depth[0].detach().cpu(), cmap=\"jet\", vmin=0, vmax=6)\n        ax[1, 1].set_title(\"Weighted Rendered Depth\")\n        ax[0, 2].imshow(diff_rgb, cmap=\"jet\", vmin=0, vmax=0.8)\n        ax[0, 2].set_title(f\"Diff RGB, Loss: {torch.round(losses['im'])}\")\n        ax[1, 2].imshow(diff_depth, cmap=\"jet\", vmin=0, vmax=0.8)\n        ax[1, 2].set_title(f\"Diff Depth, Loss: {torch.round(losses['depth'])}\")\n        ax[0, 3].imshow(presence_sil_mask.detach().cpu(), cmap=\"gray\")\n        ax[0, 3].set_title(\"Silhouette Mask\")\n        ax[1, 3].imshow(mask[0].detach().cpu(), cmap=\"gray\")\n        ax[1, 3].set_title(\"Loss Mask\")\n        # Turn off axis\n        for i in range(2):\n            for j in range(4):\n                ax[i, j].axis('off')\n        # Set Title\n        fig.suptitle(f\"Tracking Iteration: {tracking_iteration}\", fontsize=16)\n        # Figure Tight Layout\n        fig.tight_layout()\n        os.makedirs(plot_dir, exist_ok=True)\n        plt.savefig(os.path.join(plot_dir, f\"tmp.png\"), bbox_inches='tight')\n        plt.close()\n        plot_img = cv2.imread(os.path.join(plot_dir, f\"tmp.png\"))\n        cv2.imshow('Diff Images', plot_img)\n        cv2.waitKey(1)\n        ## Save Tracking Loss Viz\n        # save_plot_dir = os.path.join(plot_dir, f\"tracking_%04d\" % iter_time_idx)\n        # os.makedirs(save_plot_dir, exist_ok=True)\n        # plt.savefig(os.path.join(save_plot_dir, f\"%04d.png\" % tracking_iteration), bbox_inches='tight')\n        # plt.close()\n\n    weighted_losses = {k: v * loss_weights[k] for k, v in losses.items()}\n    loss = sum(weighted_losses.values())\n\n    seen = radius &gt; 0\n    variables['max_2D_radius'][seen] = torch.max(radius[seen], variables['max_2D_radius'][seen])\n    variables['seen'] = seen\n    weighted_losses['loss'] = loss\n\n    return loss, variables, weighted_losses\n</code></pre>"},{"location":"src/3DGS/3DGS-based_Visual_SLAM/SplaTAM/#mapping\u9ad8\u65af\u692d\u7403\u81f4\u5bc6\u5316\u548c\u5730\u56fe\u66f4\u65b0","title":"Mapping\uff1a\u9ad8\u65af\u692d\u7403\u81f4\u5bc6\u5316\u548c\u5730\u56fe\u66f4\u65b0","text":"<p>\u4e0e Tracking \u90e8\u5206\u53ef\u89c1\u8f6e\u5ed3\u56fe\u7684\u60f3\u6cd5\u7c7b\u4f3c\uff0c\u5728\u692d\u7403\u81f4\u5bc6\u5316\u4e2d SplaTAM \u4f9d\u7136\u5f15\u5165 Mask \u6765\u8868\u793a\u54ea\u4e9b\u50cf\u7d20\u5f71\u54cd\u7684\u9ad8\u65af\u692d\u7403\u5e94\u8be5\u88ab\u81f4\u5bc6\u5316 \u2014\u2014 \u90a3\u4e48\u5f53\u524d\u5e27\u7684\u54ea\u4e9b\u6570\u636e\u5e94\u8be5\u751f\u6210\u65b0\u7684\u9ad8\u65af\u692d\u7403\u5462\uff1f\u4e00\u65b9\u9762\u5f53\u7136\u662f\u5f53\u524d\u89c6\u89d2\u770b\u8fc7\u53bb\u9ad8\u65af\u692d\u7403\u4e0d\u591f\u5bc6\u96c6\u7684\u533a\u57df\uff0c\u5373 \\(\\small S(p)&lt;0.5\\)\uff1b\u53e6\u4e00\u65b9\u9762\u5219\u662f\u9884\u6d4b\u6df1\u5ea6\u5927\u4e8e\u5b9e\u9645\u6df1\u5ea6\u7684\u4f4d\u7f6e\uff0c\u5373 \\(\\small D_{\\text{GT}}(p)&lt;D(p)\\)\uff0c\u4e5f\u5c31\u662f\u8bf4\u672c\u8be5\u6709\u7269\u4f53\u906e\u6321\u7684\u4f4d\u7f6e\u662f\u7a7a\u7740\u7684\uff0c\u800c\u4e58\u4ee5 \\(\\small\\mathcal{L}_1\\big(D(p)\\big)&gt;\\lambda\\text{MDE}\\) \u5219\u662f\u4e3a\u4e86\u4fdd\u8bc1\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\u65f6\u7684\u6df1\u5ea6\u8bef\u5dee\u786e\u5b9e\u5904\u4e8e\u4e00\u4e2a\u5f02\u5e38\u503c\uff0c\u5373\u6b64\u5904\u786e\u5b9e\u6709\u4e00\u4e2a\u6211\u4e4b\u524d\u5e76\u4e0d\u77e5\u9053\u7684\u7269\u4f53\u5b58\u5728\u3002</p> \\[ \\small M(p) = \\bigg(S(p)&lt;0.5\\bigg) + \\bigg(D_{\\text{GT}}(p)&lt;D(p)\\bigg)\\bigg(\\mathcal{L}_1\\big(D(p)\\big)&gt;\\lambda\\text{MDE}\\bigg) \\] <pre><code>''' scripts/spatam.py '''\ndef rgbd_slam(config: dict):\n    ...\n        # Densification &amp; KeyFrame-based Mapping\n        if time_idx == 0 or (time_idx+1) % config['map_every'] == 0:\n            # Densification\n            if config['mapping']['add_new_gaussians'] and time_idx &gt; 0:\n                # Setup Data for Densification\n                if seperate_densification_res:\n                    # Load RGBD frames incrementally instead of all frames\n                    densify_color, densify_depth, _, _ = densify_dataset[time_idx]\n                    densify_color = densify_color.permute(2, 0, 1) / 255\n                    densify_depth = densify_depth.permute(2, 0, 1)\n                    densify_curr_data = {'cam': densify_cam, 'im': densify_color, 'depth': densify_depth, 'id': time_idx, \n                                    'intrinsics': densify_intrinsics, 'w2c': first_frame_w2c, 'iter_gt_w2c_list': curr_gt_w2c}\n                else:\n                    densify_curr_data = curr_data\n\n                # Add new Gaussians to the scene based on the Silhouette\n                params, variables = add_new_gaussians(params, variables, densify_curr_data, \n                                                        config['mapping']['sil_thres'], time_idx,\n                                                        config['mean_sq_dist_method'], config['gaussian_distribution'])\n                post_num_pts = params['means3D'].shape[0]\n                if config['use_wandb']:\n                    wandb_run.log({\"Mapping/Number of Gaussians\": post_num_pts,\n                                    \"Mapping/step\": wandb_time_step})\n\ndef add_new_gaussians(params, variables, curr_data, sil_thres, \n                      time_idx, mean_sq_dist_method, gaussian_distribution):\n    # Silhouette Rendering\n    transformed_gaussians = transform_to_frame(params, time_idx, gaussians_grad=False, camera_grad=False)\n    depth_sil_rendervar = transformed_params2depthplussilhouette(params, curr_data['w2c'],\n                                                                 transformed_gaussians)\n    depth_sil, _, _, = Renderer(raster_settings=curr_data['cam'])(**depth_sil_rendervar)\n    silhouette = depth_sil[1, :, :]\n    non_presence_sil_mask = (silhouette &lt; sil_thres)\n    # Check for new foreground objects by using GT depth\n    gt_depth = curr_data['depth'][0, :, :]\n    render_depth = depth_sil[0, :, :]\n    depth_error = torch.abs(gt_depth - render_depth) * (gt_depth &gt; 0)\n    non_presence_depth_mask = (render_depth &gt; gt_depth) * (depth_error &gt; 50*depth_error.median())\n    # Determine non-presence mask\n    non_presence_mask = non_presence_sil_mask | non_presence_depth_mask\n    # Flatten mask\n    non_presence_mask = non_presence_mask.reshape(-1)\n\n    # Get the new frame Gaussians based on the Silhouette\n    if torch.sum(non_presence_mask) &gt; 0:\n        # Get the new pointcloud in the world frame\n        curr_cam_rot = torch.nn.functional.normalize(params['cam_unnorm_rots'][..., time_idx].detach())\n        curr_cam_tran = params['cam_trans'][..., time_idx].detach()\n        curr_w2c = torch.eye(4).cuda().float()\n        curr_w2c[:3, :3] = build_rotation(curr_cam_rot)\n        curr_w2c[:3, 3] = curr_cam_tran\n        valid_depth_mask = (curr_data['depth'][0, :, :] &gt; 0)\n        non_presence_mask = non_presence_mask &amp; valid_depth_mask.reshape(-1)\n        new_pt_cld, mean3_sq_dist = get_pointcloud(curr_data['im'], curr_data['depth'], curr_data['intrinsics'], \n                                    curr_w2c, mask=non_presence_mask, compute_mean_sq_dist=True,\n                                    mean_sq_dist_method=mean_sq_dist_method)\n        new_params = initialize_new_params(new_pt_cld, mean3_sq_dist, gaussian_distribution)\n        for k, v in new_params.items():\n            params[k] = torch.nn.Parameter(torch.cat((params[k], v), dim=0).requires_grad_(True))\n        num_pts = params['means3D'].shape[0]\n        variables['means2D_gradient_accum'] = torch.zeros(num_pts, device=\"cuda\").float()\n        variables['denom'] = torch.zeros(num_pts, device=\"cuda\").float()\n        variables['max_2D_radius'] = torch.zeros(num_pts, device=\"cuda\").float()\n        new_timestep = time_idx*torch.ones(new_pt_cld.shape[0],device=\"cuda\").float()\n        variables['timestep'] = torch.cat((variables['timestep'],new_timestep),dim=0)\n\n    return params, variables\n</code></pre> <p>\u81f3\u4e8e Mapping\uff0c\u5219\u662f\u5728\u56fa\u5b9a\u76f8\u673a\u4f4d\u59ff\u7684\u60c5\u51b5\u4e0b\u4f9d\u636e\u5173\u952e\u5e27\u4f18\u5316\u9ad8\u65af\u53c2\u6570\u3002SplaTAM \u7684\u5173\u952e\u5e27\u9009\u53d6\u7b56\u7565\u662f\u9009\u62e9\u5f53\u524d\u5e27\u3001\u6700\u8fd1\u7684\u5173\u952e\u5e27\u548c\u4e0e\u5f53\u524d\u5e27\u91cd\u53e0\u6700\u591a\u7684 \\(\\small k\u22122\\) \u4e2a\u5148\u524d\u5173\u952e\u5e27\u5b58\u4e3a\u5217\u8868\u4f5c\u4f18\u5316\u3002</p> <pre><code>''' scripts/spatam.py '''\ndef rgbd_slam(config: dict):\n    ...\n            with torch.no_grad():\n                # Get the current estimated rotation &amp; translation\n                curr_cam_rot = F.normalize(params['cam_unnorm_rots'][..., time_idx].detach())\n                curr_cam_tran = params['cam_trans'][..., time_idx].detach()\n                curr_w2c = torch.eye(4).cuda().float()\n                curr_w2c[:3, :3] = build_rotation(curr_cam_rot)\n                curr_w2c[:3, 3] = curr_cam_tran\n                # Select Keyframes for Mapping\n                num_keyframes = config['mapping_window_size']-2\n                selected_keyframes = keyframe_selection_overlap(depth, curr_w2c, intrinsics, keyframe_list[:-1], num_keyframes)\n                selected_time_idx = [keyframe_list[frame_idx]['id'] for frame_idx in selected_keyframes]\n                if len(keyframe_list) &gt; 0:\n                    # Add last keyframe to the selected keyframes\n                    selected_time_idx.append(keyframe_list[-1]['id'])\n                    selected_keyframes.append(len(keyframe_list)-1)\n                # Add current frame to the selected keyframes\n                selected_time_idx.append(time_idx)\n                selected_keyframes.append(-1)\n                # Print the selected keyframes\n                print(f\"\\nSelected Keyframes at Frame {time_idx}: {selected_time_idx}\")\n</code></pre> <p>\u9009\u53d6\u5173\u952e\u5e27\u540e\uff0cMapping \u90e8\u5206\u7684\u4ee3\u7801\u4e0e Tracking \u7c7b\u4f3c\uff0c\u53ea\u662f <code>get_loss</code> \u51fd\u6570\u4f20\u5165\u7684\u6570\u636e\u4ece <code>tracking_curr_data</code> \u6362\u4e3a\u4e86 <code>iter_data</code>\uff0c\u5f85\u4f18\u5316\u7684\u53c2\u6570\u4ece\u76f8\u673a\u4f4d\u59ff\u6362\u4e3a\u4e86\u5173\u952e\u5e27\u5bf9\u5e94\u9ad8\u65af\u692d\u7403\u7684\u53c2\u6570\u3002</p> <pre><code>''' scripts/spatam.py '''\ndef rgbd_slam(config: dict):\n    ...\n            # Mapping\n            mapping_start_time = time.time()\n            if num_iters_mapping &gt; 0:\n                progress_bar = tqdm(range(num_iters_mapping), desc=f\"Mapping Time Step: {time_idx}\")\n            for iter in range(num_iters_mapping):\n                iter_start_time = time.time()\n                # Randomly select a frame until current time step amongst keyframes\n                rand_idx = np.random.randint(0, len(selected_keyframes))\n                selected_rand_keyframe_idx = selected_keyframes[rand_idx]\n                if selected_rand_keyframe_idx == -1:\n                    # Use Current Frame Data\n                    iter_time_idx = time_idx\n                    iter_color = color\n                    iter_depth = depth\n                else:\n                    # Use Keyframe Data\n                    iter_time_idx = keyframe_list[selected_rand_keyframe_idx]['id']\n                    iter_color = keyframe_list[selected_rand_keyframe_idx]['color']\n                    iter_depth = keyframe_list[selected_rand_keyframe_idx]['depth']\n                iter_gt_w2c = gt_w2c_all_frames[:iter_time_idx+1]\n                iter_data = {'cam': cam, 'im': iter_color, 'depth': iter_depth, 'id': iter_time_idx, \n                             'intrinsics': intrinsics, 'w2c': first_frame_w2c, 'iter_gt_w2c_list': iter_gt_w2c}\n                # Loss for current frame\n                loss, variables, losses = get_loss(params, iter_data, variables, iter_time_idx, config['mapping']['loss_weights'],\n                                                config['mapping']['use_sil_for_loss'], config['mapping']['sil_thres'],\n                                                config['mapping']['use_l1'], config['mapping']['ignore_outlier_depth_loss'], mapping=True)\n                if config['use_wandb']:\n                    # Report Loss\n                    wandb_mapping_step = report_loss(losses, wandb_run, wandb_mapping_step, mapping=True)\n                # Backprop\n                loss.backward()\n</code></pre> <p></p> <p> </p> <p>[1] Keetha N, Karhade J, Jatavallabhula K M, et al. Splatam: Splat track &amp; map 3d gaussians for dense rgb-d slam[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 21357-21366.</p>"},{"location":"src/3DGS/gaussian-splatting/gaussian-splatting/","title":"3D Gaussian Splatting","text":"<p>3DGS<sup>[1]</sup> \u662f\u57fa\u4e8e Splatting \u548c\u673a\u5668\u5b66\u4e60\u7684\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u3002\u5176\u4e2d Splat \u662f\u62df\u58f0\u8bcd\uff0c\u610f\u4e3a\u201c\u556a\u53fd\u4e00\u58f0\u201d\uff1a\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u7684\u8f93\u5165\u662f\u4e00\u4e9b\u96ea\u7403\uff0c\u56fe\u7247\u662f\u4e00\u9762\u7816\u5899\uff0c\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u5c31\u662f\u5411\u5899\u9762\u6254\u96ea\u7403\u7684\u8fc7\u7a0b\uff1b\u6bcf\u6254\u4e00\u4e2a\u96ea\u7403\uff0c\u5899\u9762\u4e0a\u4f1a\u7559\u6709\u5370\u8bb0\uff0c\u5e76\u4f34\u6709\u556a\u53fd\u4e00\u58f0\uff0c\u6240\u4ee5\u8fd9\u4e2a\u7b97\u6cd5\u4e5f\u88ab\u79f0\u4e3a\u629b\u96ea\u7403\u6cd5\uff0c\u7ffb\u8bd1\u6210\u201c\u55b7\u6e85\u201d\u4e5f\u5f88\u6709\u7075\u6027\u3002\u7b80\u5355\u6765\u8bf4\uff0csplatting \u7684\u6838\u5fc3\u6709\u4e09\u6b65\uff1a\u4e00\u662f\u9009\u62e9\u201c\u96ea\u7403\u201d\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u8981\u5c06\u5b83\u634f\u6210\u4e00\u4e2a\u4ec0\u4e48\u5f62\u72b6\u7684\u96ea\u7403\uff1b\u4e8c\u662f\u53bb\u629b\u63b7\u96ea\u7403\uff0c\u5c06\u9ad8\u65af\u692d\u7403\u4ece 3D \u6295\u5f71\u5230 2D\uff0c\u5f97\u5230\u5f88\u591a\u4e2a\u5370\u8bb0\uff1b\u4e09\u662f\u5408\u6210\u8fd9\u4e9b\u5370\u8bb0\u4ee5\u5f62\u6210\u6700\u540e\u7684\u56fe\u50cf<sup>[2]</sup>\u3002</p>"},{"location":"src/3DGS/gaussian-splatting/gaussian-splatting/#\u634f\u96ea\u7403\u7528\u534f\u65b9\u5dee\u63a7\u5236\u692d\u7403\u5f62\u72b6","title":"\u634f\u96ea\u7403\uff1a\u7528\u534f\u65b9\u5dee\u63a7\u5236\u692d\u7403\u5f62\u72b6","text":"<p>3DGS \u7684\u8f93\u5165\u662f SfM \u5f97\u5230\u7684\u7a00\u758f\u70b9\u4e91\uff0c\u800c\u7531\u4e8e\u70b9\u662f\u6ca1\u6709\u4f53\u79ef\u7684\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u5c06\u70b9\u81a8\u80c0\u6210\u6b63\u65b9\u4f53\u3001\u7403\u4f53\u6216\u8005\u5176\u4ed6\u57fa\u7840\u7684\u4e09\u7ef4\u51e0\u4f55\u5f62\u72b6\u3002\u4e4b\u6240\u4ee5\u9009\u62e9\u9ad8\u65af\u5206\u5e03\u4f5c\u4e3a\u692d\u7403\uff0c\u5219\u662f\u56e0\u4e3a\u5b83\u826f\u597d\u7684\u6570\u5b66\u6027\u8d28\uff0c\u6bd4\u5982\u9ad8\u65af\u5206\u5e03\u5728\u4eff\u5c04\u53d8\u6362\u540e\u4f9d\u7136\u662f\u9ad8\u65af\u5206\u5e03\uff0c\u800c\u6cbf\u7740\u67d0\u4e2a\u8f74\u79ef\u5206\u5c06\u9ad8\u65af\u5206\u5e03\u4ece 3D \u964d\u5230 2D \u540e\u5176\u4f9d\u7136\u670d\u4ece\u9ad8\u65af\u5206\u5e03\u3002\u9ad8\u65af\u5206\u5e03\u7684\u6570\u5b66\u63cf\u8ff0\u5982\u4e0b\uff1a</p> \\[ \\small G(x;\\mu,\\Sigma) = \\cfrac{1}{\\sqrt{(2\\pi)^k|\\Sigma|}}\\exp\\bigg(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)  % = \\cfrac{1}{\\sqrt{(2\\pi)^3|\\Sigma|}}\\exp\\Bigg(-\\frac{1}{2}\\bigg(\\frac{(x-\\mu_1)^2}{\\sigma_1^2}+\\frac{(y-\\mu_2)^2}{\\sigma_2^2}+\\frac{(z-\\mu_3)^2}{\\sigma_3^2}-\\frac{2\\sigma_{xy}(x-\\mu_1)(y-\\mu_2)}{\\sigma_1\\sigma_2}-\\frac{2\\sigma_{xz}(x-\\mu_1)(z-\\mu_3)}{\\sigma_1\\sigma_3}-\\frac{2\\sigma_{yz}(y-\\mu_2)(z-\\mu_3)}{\\sigma_2\\sigma_3}\\bigg)\\Bigg) \\] <p>\u540c\u65f6\uff0c\u4efb\u610f\u692d\u7403\u90fd\u53ef\u4ee5\u7531\u67d0\u4e2a\u692d\u7403\u7ecf\u8fc7\u4eff\u5c04\u53d8\u6362\u5f97\u5230\uff08\u8fd9\u5176\u5b9e\u5bf9\u5e94\u4e8e\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u89c2\u6d4b\u53d8\u6362\uff0c\u6240\u8c13\u201c\u6a2a\u770b\u6210\u5cad\u4fa7\u6210\u5cf0\uff0c\u8fdc\u8fd1\u9ad8\u4f4e\u5404\u4e0d\u540c\u201d\uff0c\u5728\u8fd9\u91cc\u6307\u7684\u5c31\u662f\u4e0d\u540c\u89c6\u89d2\u4e0b\u770b\u5230\u7684\u692d\u7403\u5f62\u72b6\u662f\u4e0d\u540c\u7684\uff09\uff0c\u800c\u4eff\u5c04\u53d8\u6362\u5de6\u4e58\u7684\u77e9\u9635 \\(\\small W\\) \u53ef\u4ee5\u89c6\u4e3a\u65cb\u8f6c\u548c\u7f29\u653e\u8fd9\u4e24\u4e2a\u4f5c\u7528\u7684\u5408\u6210\uff0c\u5373 \\(\\small W=RS\\)\uff1a</p> \\[ \\small y = Wx+b = RSx+b,\\thinspace\\thinspace x\\sim N(\\mu,\\Sigma) \\thinspace\\thinspace\\thinspace\\thinspace\\thinspace\\Longrightarrow\\thinspace\\thinspace\\thinspace\\thinspace\\thinspace y\\sim N(W\\mu+b, W\\Sigma W^T) = N(W\\mu+b, RS\\Sigma S^TR^T) \\] <p>\u7279\u522b\u5730\uff0c\u5f53 \\(\\small x\\) \u670d\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u65f6\uff0c\u4eff\u5c04\u53d8\u6362\u5f97\u5230\u7684\u534f\u65b9\u5dee\u77e9\u9635\u4e3a \\(\\small RSS^TR^T\\)\uff1b\u53cd\u8fc7\u6765\uff0c\u7ed9\u5b9a\u534f\u65b9\u5dee\u77e9\u9635 \\(\\small\\Sigma\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u7279\u5f81\u5206\u89e3\u5f97\u5230 \\(\\small R\\) \u548c \\(\\small S\\)\uff0c\u5373 \\(\\small\\Sigma=Q\\wedge Q^T=Q\\wedge^{1/2}\\wedge^{1/2} Q^T:=RSS^TR^T\\) (\u7531\u6b64\u53ef\u77e5\u5b58\u50a8\u4e00\u4e2a\u534f\u65b9\u5dee\u77e9\u9635\u9700\u8981\u4e03\u4e2a\u53c2\u6570\uff0c\u5373\u56db\u5143\u6570\u548c\u4e09\u4e2a\u7f29\u653e\u53c2\u6570)\u3002\u4e0b\u9762\u7684 <code>computeCov3D</code> \u51fd\u6570\u5c31\u5728\u8bb2\u8fd9\u4e2a\u4eff\u5c04\u53d8\u6362\uff0c\u4f20\u5165\u7684\u4e09\u7ef4\u5411\u91cf <code>scale</code> \u5373\u4e3a\u4e0a\u8ff0\u516c\u5f0f\u4e2d\u7684 \\(\\small x\\)\uff0c <code>cov3D</code> \u5219\u8868\u793a\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u53ea\u662f\u4f20\u5165\u7684\u56db\u5143\u6570 <code>rot4</code> \u4f7f\u5f97\u4ee3\u7801\u591a\u4e86\u4e00\u4e2a\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\u7684\u8fc7\u7a0b\u3002</p> <pre><code>/* submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu */\n// Forward method for converting scale and rotation properties of each Gaussian to \n// a 3D covariance matrix in world space. Also takes care of quaternion normalization.\n__device__ void computeCov3D(const glm::vec3 scale, float mod, const glm::vec4 rot, float* cov3D)\n{\n    // Create scaling matrix\n    glm::mat3 S = glm::mat3(1.0f);\n    S[0][0] = mod * scale.x;\n    S[1][1] = mod * scale.y;\n    S[2][2] = mod * scale.z;\n\n    // Normalize quaternion to get valid rotation\n    glm::vec4 q = rot;\n    float r = q.x;\n    float x = q.y;\n    float y = q.z;\n    float z = q.w;\n\n    // Compute rotation matrix from quaternion\n    glm::mat3 R = glm::mat3(\n        1.f - 2.f * (y * y + z * z), 2.f * (x * y - r * z), 2.f * (x * z + r * y),\n        2.f * (x * y + r * z), 1.f - 2.f * (x * x + z * z), 2.f * (y * z - r * x),\n        2.f * (x * z - r * y), 2.f * (y * z + r * x), 1.f - 2.f * (x * x + y * y)\n    );\n\n    glm::mat3 M = S * R;\n\n    // Compute 3D world covariance matrix Sigma\n    glm::mat3 Sigma = glm::transpose(M) * M;\n\n    // Covariance is symmetric, only store upper right\n    cov3D[0] = Sigma[0][0];\n    cov3D[1] = Sigma[0][1];\n    cov3D[2] = Sigma[0][2];\n    cov3D[3] = Sigma[1][1];\n    cov3D[4] = Sigma[1][2];\n    cov3D[5] = Sigma[2][2];\n}\n</code></pre>"},{"location":"src/3DGS/gaussian-splatting/gaussian-splatting/#\u629b\u96ea\u7403\u5c06\u4e09\u7ef4\u692d\u7403\u6295\u5f71\u5230\u4e8c\u7ef4","title":"\u629b\u96ea\u7403\uff1a\u5c06\u4e09\u7ef4\u692d\u7403\u6295\u5f71\u5230\u4e8c\u7ef4","text":"<p>\u4ece\u4e16\u754c\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u89c2\u6d4b\u53d8\u6362\u901a\u8fc7\u4e0a\u9762\u7684\u4eff\u5c04\u53d8\u6362\u63cf\u8ff0\uff0c\u800c\u76f8\u673a\u5750\u6807\u7cfb\u5230\u5f52\u4e00\u5316\u5750\u6807\u7cfb\u7684\u6295\u5f71\u53d8\u6362\u5374\u5e76\u4e0d\u662f\u4e00\u4e2a\u7ebf\u6027\u53d8\u6362\uff0c\u5b83\u9700\u8981\u5c06\u89c6\u9525\u7684\u5c41\u80a1\u538b\u6241\u5e76\u538b\u6210\u6b63\u65b9\u4f53\uff08\u8fd9\u6837\u4e00\u6765\u4e5f\u5c06\u5c04\u7ebf\u4e0e\u5750\u6807\u8f74\u5e73\u884c\u5bf9\u9f50\uff0c\u4f7f\u5f97\u6cbf\u5c04\u7ebf\u7684\u79ef\u5206\u8ba1\u7b97\u53d8\u5f97\u66f4\u52a0\u65b9\u4fbf\uff09\uff0c\u6240\u4ee5\u6211\u4eec\u8003\u8651\u5f15\u5165\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u8be5\u975e\u7ebf\u6027\u53d8\u6362\u4f5c\u5c40\u90e8\u7ebf\u6027\u8fd1\u4f3c\uff0c\u4e5f\u5c31\u662f\u7528\u4eff\u5c04\u53d8\u6362\u6765\u8fd1\u4f3c\u5c40\u90e8\u7684\u6295\u5f71\u4f5c\u7528\u3002\u90a3\u4e48\uff0c\u5728\u8be5\u538b\u6241\u7684\u5c04\u7ebf\u5750\u6807\u7cfb\u4e0b\u7684\u534f\u65b9\u5dee\u77e9\u9635\u4e3a \\(\\small\\Sigma_{ray}=JW\\Sigma W^TJ^T\\)\uff0c\u800c\u5747\u503c\u672c\u8eab\u4fbf\u662f\u9ad8\u65af\u692d\u7403\u7684\u4e2d\u5fc3\u70b9\uff0c\u53ef\u76f4\u63a5\u5bf9\u5176\u5e94\u7528\u6295\u5f71\u53d8\u6362\u3002\u5982\u6b64\uff0c\u518d\u5bf9\u6295\u5f71\u540e\u7684\u9ad8\u65af\u692d\u5706\u4f5c\u89c6\u53e3\u53d8\u6362\u4fbf\u53ef\u5f97\u5230\u5176\u5728\u50cf\u7d20\u5750\u6807\u7cfb\u4e0b\u7684\u8868\u793a\u3002</p> <p></p> <p>\u6b63\u56e0\u4e3a\u662f\u5c40\u90e8\u7ebf\u6027\u8fd1\u4f3c\uff0c\u6240\u4ee5\u4e0b\u9762\u6295\u5f71\u53d8\u6362\u7684 <code>computeCov2D</code> \u51fd\u6570\u9700\u8981\u5148\u8ba1\u7b97\u9ad8\u65af\u692d\u7403\u5747\u503c\u70b9\u5728\u89c6\u9525\u4e2d\u7684\u4f4d\u7f6e\uff1b\u4e5f\u6b63\u56e0\u4e3a\u89c6\u9525\u538b\u6241\u540e\u7684\u6b63\u4ea4\u6295\u5f71\u4e0e \\(\\small z\\) \u65b9\u5411\u65e0\u5173\uff0c\u6240\u4ee5\u5b9e\u9645\u4e0a\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u7b2c\u4e09\u884c\u662f\u53ef\u4ee5\u7f6e\u96f6\u7684\u3002</p> <pre><code>/* submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu */\n// Forward version of 2D covariance matrix computation\n__device__ float3 computeCov2D(const float3&amp; mean, float focal_x, float focal_y, float tan_fovx, float tan_fovy, const float* cov3D, const float* viewmatrix)\n{\n\t// The following models the steps outlined by equations 29 and 31 in \"EWA Splatting\" (Zwicker et al., 2002). \n\t// Additionally considers aspect / scaling of viewport.\n\t// Transposes used to account for row-/column-major conventions.\n\tfloat3 t = transformPoint4x3(mean, viewmatrix);\n\n\tconst float limx = 1.3f * tan_fovx;\n\tconst float limy = 1.3f * tan_fovy;\n\tconst float txtz = t.x / t.z;\n\tconst float tytz = t.y / t.z;\n\tt.x = min(limx, max(-limx, txtz)) * t.z;\n\tt.y = min(limy, max(-limy, tytz)) * t.z;\n\n\tglm::mat3 J = glm::mat3(\n\t\tfocal_x / t.z, 0.0f, -(focal_x * t.x) / (t.z * t.z),\n\t\t0.0f, focal_y / t.z, -(focal_y * t.y) / (t.z * t.z),\n\t\t0, 0, 0);\n\n\tglm::mat3 W = glm::mat3(\n\t\tviewmatrix[0], viewmatrix[4], viewmatrix[8],\n\t\tviewmatrix[1], viewmatrix[5], viewmatrix[9],\n\t\tviewmatrix[2], viewmatrix[6], viewmatrix[10]);\n\n\tglm::mat3 T = W * J;\n\n\tglm::mat3 Vrk = glm::mat3(\n\t\tcov3D[0], cov3D[1], cov3D[2],\n\t\tcov3D[1], cov3D[3], cov3D[4],\n\t\tcov3D[2], cov3D[4], cov3D[5]);\n\n\tglm::mat3 cov = glm::transpose(T) * glm::transpose(Vrk) * T;\n\n\treturn { float(cov[0][0]), float(cov[0][1]), float(cov[1][1]) };\n}\n</code></pre>"},{"location":"src/3DGS/gaussian-splatting/gaussian-splatting/#\u96ea\u7403\u989c\u8272\u548c\u50cf\u7d20\u7740\u8272\u7403\u8c10\u51fd\u6570","title":"\u96ea\u7403\u989c\u8272\u548c\u50cf\u7d20\u7740\u8272\uff1a\u7403\u8c10\u51fd\u6570","text":"<p>\u901a\u8fc7\u4e0a\u8ff0\u8fc7\u7a0b\uff0c\u6211\u4eec\u5df2\u7ecf\u634f\u597d\u4e86\u96ea\u7403\uff0c\u4e5f\u60f3\u597d\u5982\u4f55\u628a\u96ea\u7403\u5f80\u5899\u4e0a\u7838\u4e86\uff0c\u4f46\u96ea\u7403\u4e0d\u4e00\u5b9a\u662f\u767d\u8272\u7684 \u2014\u2014 3DGS \u5229\u7528\u7403\u8c10\u51fd\u6570 \\(\\small\\sum_l\\sum_{m=-l}^l c_l^my_l^m(\\theta,\\phi)\\) \u6765\u8868\u8fbe\u9ad8\u65af\u692d\u7403\u7684\u989c\u8272\u3002\u76f8\u6bd4 RGB \u4fe1\u606f\uff08\u5bf9\u5e94\u4e8e\u96f6\u9636\u7403\u8c10\u51fd\u6570\uff09\uff0c\u9ad8\u9636\u7403\u8c10\u51fd\u6570\u7ed9\u51fa\u4e86\u66f4\u4e3a\u903c\u771f\u7684\u73af\u5883\u8d34\u56fe\u548c\u4eae\u5ea6\u91cd\u5efa\u6548\u679c\uff0c\u4f7f\u5f97\u692d\u7403\u5448\u73b0\u7684\u989c\u8272\u4e0e\u89c2\u6d4b\u65b9\u5411\u76f8\u5173 \u2014\u2014 \u76f4\u89c9\u4e0a\u8bb2\u7403\u8c10\u51fd\u6570\u5305\u542b\u4e86\u66f4\u4e3a\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u6bd4\u5982\u4e09\u9636\u7403\u8c10\u51fd\u6570\u6240\u5305\u542b\u7684\u4fe1\u606f\u91cf\u8fbe\u5230\u4e86 \\(\\small (1+3+5+7)\\times 3\\)\u3002\u4e0b\u9762\u4ee3\u7801\u4f20\u5165\u7684\u53c2\u6570 <code>deg</code> \u5373\u4e3a\u7403\u8c10\u51fd\u6570\u7684\u9636\u6570\uff0c<code>glm::vec3 result = SH_C0 * sh[0]</code> \u4fbf\u662f\u5728\u7b97\u7b2c\u96f6\u9636\u7684\u5143\u7d20\uff0c\u540e\u7eed\u5219\u6309\u516c\u5f0f\u5206\u522b\u8ba1\u7b97\u4e0d\u540c\u9636\u6b21\u7684\u7403\u8c10\u51fd\u6570\u503c\u3002</p> <pre><code>/* submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu */\n// Forward method for converting the input spherical harmonics coefficients of each Gaussian to a simple RGB color.\n__device__ glm::vec3 computeColorFromSH(int idx, int deg, int max_coeffs, const glm::vec3* means, glm::vec3 campos, const float* shs, bool* clamped)\n{\n\t// The implementation is loosely based on code for \n\t// \"Differentiable Point-Based Radiance Fields for \n\t// Efficient View Synthesis\" by Zhang et al. (2022)\n\tglm::vec3 pos = means[idx];\n\tglm::vec3 dir = pos - campos;\n\tdir = dir / glm::length(dir);\n\n\tglm::vec3* sh = ((glm::vec3*)shs) + idx * max_coeffs;\n\tglm::vec3 result = SH_C0 * sh[0];\n\n\tif (deg &gt; 0)\n\t{\n\t\tfloat x = dir.x;\n\t\tfloat y = dir.y;\n\t\tfloat z = dir.z;\n\t\tresult = result - SH_C1 * y * sh[1] + SH_C1 * z * sh[2] - SH_C1 * x * sh[3];\n\n\t\tif (deg &gt; 1)\n\t\t{\n\t\t\tfloat xx = x * x, yy = y * y, zz = z * z;\n\t\t\tfloat xy = x * y, yz = y * z, xz = x * z;\n\t\t\tresult = result +\n\t\t\t\tSH_C2[0] * xy * sh[4] +\n\t\t\t\tSH_C2[1] * yz * sh[5] +\n\t\t\t\tSH_C2[2] * (2.0f * zz - xx - yy) * sh[6] +\n\t\t\t\tSH_C2[3] * xz * sh[7] +\n\t\t\t\tSH_C2[4] * (xx - yy) * sh[8];\n\n\t\t\tif (deg &gt; 2)\n\t\t\t{\n\t\t\t\tresult = result +\n\t\t\t\t\tSH_C3[0] * y * (3.0f * xx - yy) * sh[9] +\n\t\t\t\t\tSH_C3[1] * xy * z * sh[10] +\n\t\t\t\t\tSH_C3[2] * y * (4.0f * zz - xx - yy) * sh[11] +\n\t\t\t\t\tSH_C3[3] * z * (2.0f * zz - 3.0f * xx - 3.0f * yy) * sh[12] +\n\t\t\t\t\tSH_C3[4] * x * (4.0f * zz - xx - yy) * sh[13] +\n\t\t\t\t\tSH_C3[5] * z * (xx - yy) * sh[14] +\n\t\t\t\t\tSH_C3[6] * x * (xx - 3.0f * yy) * sh[15];\n\t\t\t}\n\t\t}\n\t}\n\tresult += 0.5f;\n\n\t// RGB colors are clamped to positive values. If values are\n\t// clamped, we need to keep track of this for the backward pass.\n\tclamped[3 * idx + 0] = (result.x &lt; 0);\n\tclamped[3 * idx + 1] = (result.y &lt; 0);\n\tclamped[3 * idx + 2] = (result.z &lt; 0);\n\treturn glm::max(result, 0.0f);\n}\n</code></pre> <p>\\(\\small\\alpha-blending\\) \u4e2d\u7684\u50cf\u7d20\u989c\u8272\u662f\u901a\u8fc7\u6cbf\u5c04\u7ebf\u7684\u4f53\u6e32\u67d3\u5f97\u5230\u7684\uff0c\u5373\u5c06\u9ad8\u65af\u692d\u7403\u6309\u7167\u5c04\u7ebf\u5750\u6807\u7cfb\u7684\u6df1\u5ea6\u6392\u5e8f\uff0c\u7136\u540e\u6309\u4ece\u8fd1\u5230\u8fdc\u7684\u987a\u5e8f\u4f9d\u6b21\u629b\u51fa\uff1a\\(\\small C=\\sum_{i=1}^N T_i\\alpha_ic_i=\\sum_{i=1}^N\\prod_{j=1}^{i-1}(1-\\alpha_i)\\big(1-\\exp(-\\sigma_i\\delta_i)\\big)c_i\\)\uff0c\u5176\u4e2d \\(\\small T(s)\\) \u8868\u793a\u5728 \\(\\small s\\) \u70b9\u4e4b\u524d\u5149\u7ebf\u6ca1\u6709\u88ab\u963b\u788d\u7684\u6982\u7387\u6216\u8005\u8bf4\u900f\u8fc7\u7387\uff0c\\(\\small\\sigma(s)\\) \u8868\u793a\u5728 \\(\\small s\\) \u70b9\u5904\u5149\u7ebf\u649e\u51fb\u7c92\u5b50\u6216\u8005\u8bf4\u88ab\u7c92\u5b50\u963b\u788d\u7684\u6982\u7387\uff0c\\(\\small c(s)\\) \u8868\u793a\u5728 \\(\\small s\\) \u70b9\u5904\u7c92\u5b50\u53d1\u51fa\u7684\u989c\u8272\uff0c\\(\\small\\delta(s)\\) \u5219\u8868\u793a\u70b9 \\(\\small s\\) \u5904\u6cbf\u5c04\u7ebf\u79bb\u6563\u79ef\u5206\u7684\u95f4\u8ddd\u3002\u4e0b\u9762\u4ee3\u7801\u7684\u9ad8\u4eae\u90e8\u5206\u5bf9\u5e94\u7684\u5c31\u662f\u4e0a\u8ff0\u516c\u5f0f\u3002</p> <pre><code>/* submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu */\n// Main rasterization method. Collaboratively works on one tile per block, \n// each thread treats one pixel. Alternates between fetching and rasterizing data.\ntemplate &lt;uint32_t CHANNELS&gt;\n__global__ void __launch_bounds__(BLOCK_X * BLOCK_Y)\nrenderCUDA(\n\tconst uint2* __restrict__ ranges,\n\tconst uint32_t* __restrict__ point_list,\n\tint W, int H,\n\tconst float2* __restrict__ points_xy_image,\n\tconst float* __restrict__ features,\n\tconst float4* __restrict__ conic_opacity,\n\tfloat* __restrict__ final_T,\n\tuint32_t* __restrict__ n_contrib,\n\tconst float* __restrict__ bg_color,\n\tfloat* __restrict__ out_color,\n\tconst float* __restrict__ depths,\n\tfloat* __restrict__ invdepth)\n{\n\t// Identify current tile and associated min/max pixel range.\n\tauto block = cg::this_thread_block();\n\tuint32_t horizontal_blocks = (W + BLOCK_X - 1) / BLOCK_X;\n\tuint2 pix_min = { block.group_index().x * BLOCK_X, block.group_index().y * BLOCK_Y };\n\tuint2 pix_max = { min(pix_min.x + BLOCK_X, W), min(pix_min.y + BLOCK_Y , H) };\n\tuint2 pix = { pix_min.x + block.thread_index().x, pix_min.y + block.thread_index().y };\n\tuint32_t pix_id = W * pix.y + pix.x;\n\tfloat2 pixf = { (float)pix.x, (float)pix.y };\n\n\t// Check if this thread is associated with a valid pixel or outside.\n\tbool inside = pix.x &lt; W&amp;&amp; pix.y &lt; H;\n\t// Done threads can help with fetching, but don't rasterize\n\tbool done = !inside;\n\n\t// Load start/end range of IDs to process in bit sorted list.\n\tuint2 range = ranges[block.group_index().y * horizontal_blocks + block.group_index().x];\n\tconst int rounds = ((range.y - range.x + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\tint toDo = range.y - range.x;\n\n\t// Allocate storage for batches of collectively fetched data.\n\t__shared__ int collected_id[BLOCK_SIZE];\n\t__shared__ float2 collected_xy[BLOCK_SIZE];\n\t__shared__ float4 collected_conic_opacity[BLOCK_SIZE];\n\n\t// Initialize helper variables\n\tfloat T = 1.0f;\n\tuint32_t contributor = 0;\n\tuint32_t last_contributor = 0;\n\tfloat C[CHANNELS] = { 0 };\n\n\tfloat expected_invdepth = 0.0f;\n\n\t// Iterate over batches until all done or range is complete\n\tfor (int i = 0; i &lt; rounds; i++, toDo -= BLOCK_SIZE)\n\t{\n\t\t// End if entire block votes that it is done rasterizing\n\t\tint num_done = __syncthreads_count(done);\n\t\tif (num_done == BLOCK_SIZE)\n\t\t\tbreak;\n\n\t\t// Collectively fetch per-Gaussian data from global to shared\n\t\tint progress = i * BLOCK_SIZE + block.thread_rank();\n\t\tif (range.x + progress &lt; range.y)\n\t\t{\n\t\t\tint coll_id = point_list[range.x + progress];\n\t\t\tcollected_id[block.thread_rank()] = coll_id;\n\t\t\tcollected_xy[block.thread_rank()] = points_xy_image[coll_id];\n\t\t\tcollected_conic_opacity[block.thread_rank()] = conic_opacity[coll_id];\n\t\t}\n\t\tblock.sync();\n\n\t\t// Iterate over current batch\n\t\tfor (int j = 0; !done &amp;&amp; j &lt; min(BLOCK_SIZE, toDo); j++)\n\t\t{\n\t\t\t// Keep track of current position in range\n\t\t\tcontributor++;\n\n\t\t\t// Resample using conic matrix (cf. \"Surface \n\t\t\t// Splatting\" by Zwicker et al., 2001)\n\t\t\tfloat2 xy = collected_xy[j];\n\t\t\tfloat2 d = { xy.x - pixf.x, xy.y - pixf.y };\n\t\t\tfloat4 con_o = collected_conic_opacity[j];\n\t\t\tfloat power = -0.5f * (con_o.x * d.x * d.x + con_o.z * d.y * d.y) - con_o.y * d.x * d.y;\n\t\t\tif (power &gt; 0.0f)\n\t\t\t\tcontinue;\n\n\t\t\t// Eq. (2) from 3D Gaussian splatting paper.\n\t\t\t// Obtain alpha by multiplying with Gaussian opacity\n\t\t\t// and its exponential falloff from mean.\n\t\t\t// Avoid numerical instabilities (see paper appendix). \n\t\t\tfloat alpha = min(0.99f, con_o.w * exp(power));\n\t\t\tif (alpha &lt; 1.0f / 255.0f)\n\t\t\t\tcontinue;\n\t\t\tfloat test_T = T * (1 - alpha);\n\t\t\tif (test_T &lt; 0.0001f)\n\t\t\t{\n\t\t\t\tdone = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t// Eq. (3) from 3D Gaussian splatting paper.\n\t\t\tfor (int ch = 0; ch &lt; CHANNELS; ch++)\n\t\t\t\tC[ch] += features[collected_id[j] * CHANNELS + ch] * alpha * T;\n\n\t\t\tif(invdepth)\n\t\t\texpected_invdepth += (1 / depths[collected_id[j]]) * alpha * T;\n\n\t\t\tT = test_T;\n\n\t\t\t// Keep track of last range entry to update this\n\t\t\t// pixel.\n\t\t\tlast_contributor = contributor;\n\t\t}\n\t}\n\n\t// All threads that treat valid pixel write out their final\n\t// rendering data to the frame and auxiliary buffers.\n\tif (inside)\n\t{\n\t\tfinal_T[pix_id] = T;\n\t\tn_contrib[pix_id] = last_contributor;\n\t\tfor (int ch = 0; ch &lt; CHANNELS; ch++)\n\t\t\tout_color[ch * H * W + pix_id] = C[ch] + T * bg_color[ch];\n\n\t\tif (invdepth)\n\t\tinvdepth[pix_id] = expected_invdepth;// 1. / (expected_depth + T * 1e3);\n\t}\n}\n</code></pre>"},{"location":"src/3DGS/gaussian-splatting/gaussian-splatting/#\u5b8c\u6574\u6d41\u7a0b\u673a\u5668\u5b66\u4e60\u4e0e\u53c2\u6570\u8bc4\u4f30","title":"\u5b8c\u6574\u6d41\u7a0b\uff1a\u673a\u5668\u5b66\u4e60\u4e0e\u53c2\u6570\u8bc4\u4f30","text":"<p>\u6bcf\u4e2a\u70b9\u81a8\u80c0\u6210\u7684\u4e09\u7ef4\u9ad8\u65af\u692d\u7403\u53c2\u6570\u5305\u62ec\u4e2d\u5fc3\u70b9\u4f4d\u7f6e \\(\\small (x,y,z)\\)\u3001\u534f\u65b9\u5dee\u77e9\u9635 \\(\\small\\Sigma=RS\\)\u3001\u7403\u8c10\u51fd\u6570\u7cfb\u6570\u77e9\u9635\u548c\u900f\u660e\u5ea6 \\(\\small\\alpha\\)\uff0c\u8fd9\u4e9b\u521d\u59cb\u5316\u7684\u9ad8\u65af\u692d\u7403\u901a\u8fc7\u4e0a\u8ff0\u6cfc\u6e85\u7684\u8fc7\u7a0b\u5f97\u5230\u4e8c\u7ef4\u56fe\u50cf\uff0c\u518d\u5c06\u8be5\u56fe\u50cf\u548c Ground Truth \u7684\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\u6765\u4f18\u5316\u692d\u7403\u53c2\u6570\uff0c\u5176\u4e2d\u635f\u5931\u51fd\u6570\u88ab\u5b9a\u4e49\u4e3a \\(\\small\\mathcal{L}=(1-\\lambda)\\mathcal{L}_1 + \\lambda\\mathcal{L}_{D-SSIM}\\)\uff0c\u5982\u4e0b\u8ff0\u4ee3\u7801\u5757\u6240\u793a\uff08\u53ef\u4ee5\u6ce8\u610f\u5230\u4ee3\u7801\u4e2d\u8fd8\u8ba1\u7b97\u4e86\u6df1\u5ea6\u6b63\u5219\u5316\u635f\u5931\u6765\u5f15\u5bfc\u9ad8\u65af\u692d\u7403\u7684\u51e0\u4f55\u5206\u5e03\u4e0e\u5355\u76ee\u5148\u9a8c\u6df1\u5ea6\u4f30\u8ba1\u4fdd\u6301\u4e00\u81f4\uff0c\u800c\u91c7\u7528\u9006\u6df1\u5ea6\u56fe\u5219\u662f\u56e0\u4e3a\u8fd1\u5904\u7684\u6df1\u5ea6\u4f30\u8ba1\u66f4\u4e3a\u51c6\u786e\uff09\u3002\u53ef\u4ee5\u6ce8\u610f\u5230\u7684\u662f\uff0c\u4ee3\u7801 <code>submodules</code> \u6a21\u5757\u4e0b\u6709 <code>simple-knn</code> \u90e8\u5206\uff0c\u8fd9\u662f\u56e0\u4e3a\u9ad8\u65af\u692d\u7403\u88ab\u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u5404\u5411\u540c\u6027\u7684\u7403\uff0c\u5176\u534a\u5f84\u88ab\u8bbe\u4e3a\u4e09\u8fd1\u90bb\u8ddd\u79bb\u7684\u5e73\u5747\u503c\u4ee5\u907f\u514d\u692d\u7403\u94fa\u4e0d\u6ee1\u573a\u666f\u6216\u8005\u8fc7\u5ea6\u91cd\u53e0\u7684\u60c5\u51b5\u3002</p> <pre><code>''' train.py '''\ndef training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from):\n\t...\n\t# Loss\n\tgt_image = viewpoint_cam.original_image.cuda()\n\tLl1 = l1_loss(image, gt_image)\n\tif FUSED_SSIM_AVAILABLE:\n\t\tssim_value = fused_ssim(image.unsqueeze(0), gt_image.unsqueeze(0))\n\telse:\n\t\tssim_value = ssim(image, gt_image)\n\n\tloss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim_value)\n\n\t# Depth regularization\n\tLl1depth_pure = 0.0\n\tif depth_l1_weight(iteration) &gt; 0 and viewpoint_cam.depth_reliable:\n\t\tinvDepth = render_pkg[\"depth\"]\n\t\tmono_invdepth = viewpoint_cam.invdepthmap.cuda()\n\t\tdepth_mask = viewpoint_cam.depth_mask.cuda()\n\n\t\tLl1depth_pure = torch.abs((invDepth  - mono_invdepth) * depth_mask).mean()\n\t\tLl1depth = depth_l1_weight(iteration) * Ll1depth_pure\n\t\tloss += Ll1depth\n\t\tLl1depth = Ll1depth.item()\n\telse:\n\t\tLl1depth = 0\n\n\tloss.backward()\n</code></pre> <p>\u4f46\u662f\u5982\u679c\u53ea\u5bf9 colmap \u751f\u6210\u7684\u521d\u59cb\u70b9\u4e91\u4f5c\u4f18\u5316\u7684\u8bdd\uff0c\u90a3\u4e48\u540e\u7eed\u4e0d\u7ba1\u5982\u4f55\u4f18\u5316\u9ad8\u65af\u692d\u7403\u7684\u6570\u91cf\u90fd\u662f\u4e0d\u53d8\u7684\uff0c\u8fd9\u4f7f\u5f97\u7b97\u6cd5\u5f3a\u4f9d\u8d56\u4e8e SfM \u7684\u521d\u59cb\u5316\uff0c\u6240\u4ee5 3DGS \u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u4e0e\u4f18\u5316\uff0c\u5373\u5bf9\u900f\u660e\u7684\u9ad8\u65af\u5206\u5e03\u4f5c\u5468\u671f\u6027\u6ee4\u9664\u6216\u8005\u8bf4\u5254\u9664\u5b58\u5728\u611f\u592a\u4f4e\u7684\u9ad8\u65af\u692d\u7403\uff0c\u540c\u65f6\uff0c\u5bf9\u4e8e under-reconstruction \u7684\u533a\u57df\uff0c\u514b\u9686\u9ad8\u65af\u5e76\u6cbf\u7740\u68af\u5ea6\u65b9\u5411\u79fb\u52a8\u4ee5\u8986\u76d6\u51e0\u4f55\u4f53\uff1b\u5bf9\u4e8e over-reconstruction \u7684\u533a\u57df\u5219\u62c6\u5206\u9ad8\u65af\u4ee5\u66f4\u597d\u5730\u62df\u5408\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u53ef\u4ee5\u53d1\u73b0\uff0c\u673a\u5668\u5b66\u4e60\u7684\u90e8\u5206\u975e\u5e38\u7b80\u5355\u4e14\u4e0d\u6d89\u53ca\u6df1\u5ea6\u5b66\u4e60\u7684\u77e5\u8bc6\uff0c3DGS \u7684\u96be\u5ea6\u4e3b\u8981\u5728\u4e8e\u5bf9\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u7406\u89e3\u548c GPU \u7684\u9ad8\u6027\u80fd\u7f16\u7a0b\u30023DGS \u8fd9\u90e8\u5206\u7684\u4f2a\u4ee3\u7801\u89c1\u8bba\u6587\u7684\u9644\u5f55 B. Optimization and Densification Algorithm\u3002</p> <p></p> <p>\u6ce8\uff1a\u7f51\u4e0a\u6709\u53cd\u6620\u8bf4\u539f\u7248 3DGS \u5185\u7f6e\u7684\u67e5\u770b\u5668\u4e0d\u592a\u597d\u7528\uff0c\u53ef\u4ee5\u8003\u8651\u6362\u7528 gaussian-splatting-lightning\u3002</p> <p> </p> <p>[1] Kerbl B, Kopanas G, Leimk\u00fchler T, et al. 3D Gaussian splatting for real-time radiance field rendering[J]. ACM Trans. Graph., 2023, 42(4): 139:1-139:14.</p> <p>[2] Bilibili \u4e0a\u7684\u8fd9\u4e2a\u89c6\u9891\u7ed9\u51fa\u4e86 3DGS \u4eba\u6027\u5316\u7684\u8bb2\u89e3\uff0c\u4e5f\u662f\u6211\u8fd9\u7bc7\u7b14\u8bb0\u7684\u6765\u6e90\u3002</p>"},{"location":"src/Survey/paper-list/","title":"3DGS-SLAM Paper List","text":"Venue Paper Abbr Title Equipment External Tracker Extra Priors Code CVPR'24  SplaTAM              SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM           RGB-D  CVPR'24  Photo-SLAM              Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras           RGB-D, RGB, Stereo   ORB-SLAM3  ECCV'24  GS-ICP SLAM              RGBD GS-ICP SLAM           RGB-D   G-ICP   CVPR'25   WildGS-SLAM              WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments           Monocular RGB   Depth &amp; Uncertainty Guided DBA   Uncertainty Prediction  ICCV'25  SEGS-SLAM              SEGS-SLAM: Structure-enhanced 3D Gaussian Splatting SLAM with Appearance Embedding           Monocular, Stereo, RGB-D   ORB-SLAM3  ICRA'25  OpenGS-SLAM             RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes           RGB   pointmap regression network  ICCV'25  S3PO-GS SLAM             Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps           RGB   pre-trained pointmap model  arXiv'25  4D Gaussian Splatting SLAM              4D Gaussian Splatting SLAM"},{"location":"src/Survey/related-material/","title":"3DGS \u76f8\u5173\u8d44\u6599","text":"<p>3DGS SLAM \u76f8\u5173\u7684\u7efc\u8ff0\u8bba\u6587\u89c1\u4e0b\u8868\u3002</p> Venue Title  arXiv 24              How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey           IEEE Trans. Vis. Comput. Graph. 24              3D Gaussian Splatting as a New Era: A Survey           IEEE Trans. Pattern Anal. Mach. Intell. 25              A Survey on 3D Gaussian Splatting           IEEE Internet of Things Journal 25              Beyond Implicit Representations: Exploring Gaussian Splatting for Next-Generation SLAM, Introduction and Review           arXiv 25              A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation          <p>Github \u4e0a\u4e0e 3DGS SLAM \u76f8\u5173\u7684\u6587\u6863\u4ed3\u5e93\u89c1\u4e0b\u8868\u3002</p>  GitHub Repository Name   Link   awesome-NeRF-and-3DGS-SLAM   Awesome-3DGS-Applications   Awesome-3DGS-SLAM"}]}